\newpage
\section{Variables Aléatoires Discrètes}

\subsection{Variable Aléatoire}

\begin{definitionbox}[Variable Aléatoire]
Étant donné une expérience avec un univers $S$, une variable aléatoire est une fonction de l'univers $S$ vers les nombres réels $\mathbb{R}$.
\end{definitionbox}

\begin{intuitionbox}
Une variable aléatoire est une manière de traduire les résultats d'une expérience en nombres. Au lieu de travailler avec des concepts comme "Pile" ou "Face", on leur assigne des valeurs numériques (par exemple, 1 pour Pile, 0 pour Face). Cela nous permet d'utiliser toute la puissance des outils mathématiques (fonctions, calculs, etc.) pour analyser le hasard. C'est un pont entre le monde concret des événements et le monde abstrait des nombres.
\end{intuitionbox}

\begin{examplebox}
On lance deux dés. L'univers $S$ est l'ensemble des 36 paires de résultats, comme $(1,1), (1,2), \dots, (6,6)$. On peut définir une variable aléatoire $X$ comme étant la \textbf{somme des deux dés}.
Pour le résultat $(2, 5)$, la valeur de la variable aléatoire est $X(2, 5) = 2 + 5 = 7$.
\end{examplebox}

\subsection{Variable Aléatoire Discrète}

\begin{definitionbox}[Variable Aléatoire Discrète]
Une variable aléatoire $X$ est dite discrète s'il existe une liste finie ou infinie dénombrable de valeurs $a_1, a_2, \dots$ telle que $P(X=a_j \text{ pour un certain } j) = 1$.
\end{definitionbox}

\begin{intuitionbox}
Une variable aléatoire est "discrète" si on peut lister (compter) toutes les valeurs qu'elle peut prendre, même si cette liste est infinie. Pensez aux "sauts" d'une valeur à l'autre, sans possibilité de prendre une valeur intermédiaire. C'est comme monter un escalier : on peut être sur la marche 1, 2 ou 3, mais jamais sur la marche 2.5. Le nombre de têtes en 10 lancers, le résultat d'un dé, le nombre d'emails que vous recevez en une heure sont des exemples. À l'opposé, une variable continue pourrait être la taille exacte d'une personne, qui peut prendre n'importe quelle valeur dans un intervalle.
\end{intuitionbox}

\subsection{Fonction de Masse (PMF)}

\begin{definitionbox}[Probability Mass Function (PMF)]
La fonction de masse (PMF) d'une variable aléatoire discrète $X$ est la fonction $P_X$ donnée par $P_X(x) = P(X=x)$.
\end{definitionbox}

\begin{intuitionbox}
La PMF est la "carte d'identité" probabiliste d'une variable aléatoire discrète. Pour chaque valeur que la variable peut prendre, la PMF nous donne la probabilité exacte associée à cette valeur. C'est comme si chaque résultat possible avait une "étiquette de prix" qui indique sa chance de se produire. La somme de toutes ces probabilités doit bien sûr valoir 1.
\end{intuitionbox}

\begin{examplebox}
Soit $X$ le résultat d'un lancer de dé équilibré. La variable $X$ peut prendre les valeurs $\{1, 2, 3, 4, 5, 6\}$.
La PMF de $X$ est la fonction qui assigne $1/6$ à chaque valeur :
$P(X=1) = 1/6$, $P(X=2) = 1/6$, ..., $P(X=6) = 1/6$.
Pour toute autre valeur $x$ (par exemple $x=2.5$ ou $x=7$), $P(X=x) = 0$.
\end{examplebox}

\subsection{Loi de Bernoulli}

\begin{definitionbox}[Distribution de Bernoulli]
Une variable aléatoire $X$ suit la distribution de Bernoulli avec paramètre $p$ si $P(X=1) = p$ et $P(X=0) = 1-p$, où $0 < p < 1$. On note cela $X \sim \text{Bern}(p)$.
\end{definitionbox}

\begin{intuitionbox}
La distribution de Bernoulli est le modèle le plus simple pour une expérience aléatoire avec seulement deux issues : "succès" (codé par 1) et "échec" (codé par 0). C'est la brique de base de nombreuses autres distributions. Pensez à un unique lancer de pièce (Pile/Face), un unique tir au but (Marqué/Manqué), ou la réponse à une question par oui/non. Le paramètre $p$ est simplement la probabilité du "succès".
\end{intuitionbox}

\subsection{Loi Binomiale}

\begin{theorembox}[PMF Binomiale]
Si $X \sim \text{Bin}(n, p)$, alors la PMF de $X$ est :
$$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
pour $k = 0, 1, \dots, n$.
\end{theorembox}

\begin{intuitionbox}
La distribution binomiale répond à la question : "Si je répète $n$ fois la même expérience de Bernoulli (qui a une probabilité de succès $p$), quelle est la probabilité d'obtenir exactement $k$ succès ?"
La formule est construite logiquement en multipliant trois composantes. D'abord, $\mathbf{p^k}$ représente la probabilité d'obtenir $k$ succès. Ensuite, $\mathbf{(1-p)^{n-k}}$ est la probabilité que les $n-k$ échecs restants se produisent. Finalement, comme les $k$ succès peuvent apparaître n'importe où parmi les $n$ essais, on multiplie par $\mathbf{\binom{n}{k}}$, qui compte le nombre de manières distinctes de placer ces succès.
\end{intuitionbox}

\begin{examplebox}
On lance une pièce équilibrée 10 fois ($n=10$, $p=0.5$). Quelle est la probabilité d'obtenir exactement 6 Piles ($k=6$) ?
$$ P(X=6) = \binom{10}{6} (0.5)^6 (1-0.5)^{10-6} = \frac{10!}{6!4!} (0.5)^{10} = 210 \times (0.5)^{10} \approx 0.205 $$
Il y a environ 20.5\% de chance d'obtenir exactement 6 Piles.
\end{examplebox}

\subsection{Loi Hypergéométrique}

\begin{theorembox}[PMF Hypergéométrique]
Si $X \sim \text{HG}(w, b, m)$, alors la PMF de $X$ est :
$$ P(X=k) = \frac{\binom{w}{k} \binom{b}{m-k}}{\binom{w+b}{m}} $$
\end{theorembox}

\begin{intuitionbox}
La distribution hypergéométrique est la "cousine" de la binomiale pour les tirages \textbf{sans remise}. Imaginez une urne avec des boules de deux couleurs (par exemple, $w$ blanches et $b$ noires). Vous tirez $m$ boules d'un coup. Quelle est la probabilité que vous ayez exactement $k$ boules blanches ?
La formule est un simple ratio issu du dénombrement. Le \textbf{dénominateur}, $\binom{w+b}{m}$, compte le nombre total de façons de tirer $m$ boules parmi toutes celles disponibles. Le \textbf{numérateur} compte les issues favorables : c'est le produit du nombre de façons de choisir $k$ blanches parmi les $w$ ($\binom{w}{k}$) ET de choisir les $m-k$ boules restantes parmi les noires ($\binom{b}{m-k}$). La différence clé avec la loi binomiale est que les tirages ne sont pas indépendants.
\end{intuitionbox}

\begin{examplebox}
Un comité de 5 personnes est choisi au hasard parmi un groupe de 8 hommes et 10 femmes. Quelle est la probabilité que le comité soit composé de 2 hommes et 3 femmes ?
Ici, on tire 5 personnes ($m=5$) d'une population de 18 personnes. On s'intéresse au nombre d'hommes ($k=2$) parmi les 8 disponibles ($w=8$). Le reste du comité sera composé de femmes ($b=10$).
$$ P(X=2) = \frac{\binom{8}{2} \binom{10}{3}}{\binom{18}{5}} = \frac{28 \times 120}{8568} \approx 0.392 $$
Il y a environ 39.2\% de chance que le comité ait exactement cette composition.
\end{examplebox}

\subsection{Loi Géométrique}

\begin{theorembox}[PMF de la loi géométrique]
Une variable aléatoire $X$ suit la loi géométrique de paramètre $p$, notée $X \sim \text{Geom}(p)$, si elle modélise le nombre d'échecs avant le premier succès dans une série d'épreuves de Bernoulli indépendantes. Sa fonction de masse (PMF) est :
$$ P(X=k) = (1-p)^k p \quad \text{pour } k=0, 1, 2, \dots $$
où $q = 1-p$ est la probabilité d'échec.
\end{theorembox}

\begin{intuitionbox}
La formule $P(X=k) = q^k p$ décrit la probabilité d'une séquence très spécifique : $k$ échecs consécutifs (chacun avec une probabilité $q$, donc $q^k$ pour la série), suivis immédiatement d'un succès (avec une probabilité $p$). C'est la loi de "l'attente du premier succès".
\end{intuitionbox}

\begin{examplebox}[Premier 6 au lancer de dé]
On lance un dé jusqu'à obtenir un 6. La probabilité de succès est $p=1/6$, et celle d'échec est $q=5/6$. Quelle est la probabilité que l'on ait besoin de 3 lancers (donc 2 échecs avant le premier succès) ?
Ici, $k=2$. La probabilité est :
$$ P(X=2) = (5/6)^2 \cdot (1/6) = \frac{25}{216} \approx 0.116 $$
\end{examplebox}

\subsection{Loi de Poisson}

\begin{definitionbox}[Distribution de Poisson]
Une variable aléatoire $X$ suit la loi de Poisson de paramètre $\lambda > 0$ si sa PMF est donnée par :
$$ P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!} \quad \text{pour } k=0, 1, 2, \dots $$
Elle modélise typiquement le nombre d'événements se produisant dans un intervalle de temps ou d'espace fixe.
\end{definitionbox}

\begin{intuitionbox}
La loi de Poisson est la loi des événements rares. Imaginez que vous comptez le nombre d'appels arrivant à un standard téléphonique en une minute. Il y a de nombreux instants où un appel pourrait arriver, mais la probabilité à chaque instant est infime. La loi de Poisson modélise ce type de scénario, où l'on connaît seulement le taux moyen d'arrivée des événements ($\lambda$).
\end{intuitionbox}

\begin{theorembox}[La loi de Poisson comme limite de la loi binomiale]
Soit $X_n \sim \text{Bin}(n, p_n)$, où $\lambda = np_n$ est une constante positive fixée. Alors, pour tout $k \in \{0, 1, 2, \dots\}$, nous avons :
$$ \lim_{n \to \infty} P(X_n=k) = \frac{e^{-\lambda}\lambda^k}{k!} $$
En pratique, la loi de Poisson est une excellente approximation de la loi binomiale quand $n$ est grand et $p$ est petit.
\end{theorembox}

% \begin{proofbox}[Dérivation de la loi de Poisson à partir de la binomiale]
% On part de la PMF de la loi binomiale $X_n \sim \text{Bin}(n, p)$ avec $p=\lambda/n$.
% \begin{align*}
% \lim_{n \to \infty} P(X_n=k) &= \lim_{n \to \infty} \binom{n}{k} p^k (1-p)^{n-k} \\
% &= \lim_{n \to \infty} \frac{n!}{k!(n-k)!} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} \\
% &= \frac{\lambda^k}{k!} \lim_{n \to \infty} \frac{n(n-1)\cdots(n-k+1)}{n^k} \left(1-\frac{\lambda}{n}\right)^n \left(1-\frac{\lambda}{n}\right)^{-k}
% \end{align*}
% Analysons chaque terme de la limite :
% \begin{enumerate}
%     \item $\displaystyle \lim_{n \to \infty} \frac{n(n-1)\cdots(n-k+1)}{n^k} = \lim_{n \to \infty} \left(\frac{n}{n}\right)\left(\frac{n-1}{n}\right)\cdots\left(\frac{n-k+1}{n}\right) = 1$
%     \item $\displaystyle \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda}$
%     \item $\displaystyle \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = 1$
% \end{enumerate}
% En rassemblant ces résultats, on obtient :
% $$ \lim_{n \to \infty} P(X_n=k) = \frac{\lambda^k}{k!} \cdot 1 \cdot e^{-\lambda} \cdot 1 = \frac{e^{-\lambda}\lambda^k}{k!} $$
% \end{proofbox}

\begin{proofbox}[Dérivation de la loi de Poisson à partir de la loi Binomiale]
Supposons que les bébés naissent à un taux moyen de \textbf{$\lambda$} naissances par jour. On peut modéliser ce processus en divisant la journée en \textbf{$n$} très petits sous-intervalles de temps.

\vspace{0.3cm}
\noindent\textbf{1. Modèle Binomial :}
\newline
Si $n$ est suffisamment grand, on peut considérer que durant chaque petit sous-intervalle, il y a au plus une naissance. La probabilité d'une naissance durant un de ces sous-intervalles est donc $p = \lambda/n$.
\newline
Chaque sous-intervalle peut être vu comme une épreuve de Bernoulli indépendante (soit une naissance, soit pas de naissance). Le nombre total de naissances en une journée, $X$, est la somme de ces épreuves. Il suit donc une loi binomiale :
$$ X \sim \text{Bin}\left(n, p = \frac{\lambda}{n}\right) $$
La fonction de masse de probabilité (PMF) pour obtenir exactement $k$ naissances est :
$$ P(X=k) = \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} $$

\vspace{0.3cm}
\noindent\textbf{2. Passage à la limite :}
\newline
Pour modéliser un processus continu, on fait tendre le nombre de sous-intervalles $n$ vers l'infini. On examine alors la limite de la PMF binomiale :
$$ \lim_{n\to\infty} \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} $$
En utilisant les limites connues $\lim_{n\to\infty} \frac{n(n-1)\cdots(n-k+1)}{n^k} = 1$ et $\lim_{n\to\infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda}$, on peut montrer que cette expression converge vers :
$$ \frac{e^{-\lambda}\lambda^k}{k!} $$

\vspace{0.3cm}
\noindent\textbf{Conclusion :}
\newline
Lorsque le nombre d'essais ($n$) devient très grand et la probabilité de succès ($p$) très petite, la loi binomiale converge vers la loi de Poisson. Le nombre de naissances en un jour suit donc une \textbf{loi de Poisson} de paramètre $\lambda$, où $\lambda$ est le taux moyen de naissances par jour.
\end{proofbox}


\begin{examplebox}[Décès par ruade de cheval : Les données de Bortkiewicz]
En 1898, le statisticien Ladislaus Bortkiewicz a publié des données célèbres sur le nombre de soldats de la cavalerie prussienne tués par des ruades de cheval. Ces données sont un exemple classique d'application de la loi de Poisson pour modéliser des événements rares.

\vspace{0.3cm}
\noindent\textbf{Contexte et calcul du paramètre $\lambda$ :}
\newline
Sur une période de 20 ans, en observant 10 corps d'armée, il a collecté des données sur 200 "corps-années". Durant cette période, il y a eu un total de 122 décès. Le taux moyen de décès par corps-année est donc :
$$ \lambda = \frac{\text{Nombre total de décès}}{\text{Nombre total de corps-années}} = \frac{122}{200} = 0.61 $$
Le nombre de décès par corps-année, $X$, est donc modélisé par une loi de Poisson : $X \sim \text{Poisson}(\lambda=0.61)$.

\vspace{0.3cm}
\noindent\textbf{Comparaison des données observées et des prédictions du modèle :}
\newline
On peut calculer la probabilité d'observer $k$ décès en une année-corps en utilisant la PMF de Poisson : $P(X=k) = \frac{e^{-0.61} (0.61)^k}{k!}$. En multipliant cette probabilité par le nombre total d'observations (200), on obtient le nombre de cas attendus (nombre de corps d'armes dans lequels il y a k deces).

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Nombre de décès (k)} & \textbf{Observé} & \textbf{Probabilité de Poisson} & \textbf{Attendu} \\
\hline
0 & 109 & $P(X=0) \approx 0.543$ & 108.7 \\
1 & 65 & $P(X=1) \approx 0.331$ & 66.3 \\
2 & 22 & $P(X=2) \approx 0.101$ & 20.2 \\
3 & 3 & $P(X=3) \approx 0.021$ & 4.1 \\
4 & 1 & $P(X=4) \approx 0.003$ & 0.6 \\
5+ & 0 & $P(X \ge 5) \approx 0.000$ & 0.0 \\
\hline
\end{tabular}
\end{center}

\vspace{0.3cm}
L'adéquation remarquable entre les fréquences observées et les valeurs attendues par le modèle de Poisson a contribué à populariser cette distribution pour l'analyse d'événements rares.
\end{examplebox}


\subsection{Fonction de Répartition (CDF)}

\begin{definitionbox}[Cumulative Distribution Function (CDF)]
La fonction de répartition (CDF) d'une variable aléatoire $X$ est la fonction $F_X$ donnée par $F_X(x) = P(X \le x)$.
\end{definitionbox}

\begin{intuitionbox}
Alors que la PMF répond à la question "Quelle est la probabilité d'obtenir \textit{exactement} $x$ ?", la CDF répond à la question "Quelle est la probabilité d'obtenir \textit{au plus} $x$ ?". C'est une fonction cumulative : pour une valeur $x$ donnée, elle additionne les probabilités de tous les résultats inférieurs ou égaux à $x$.
La CDF a toujours une forme d'escalier pour les variables discrètes. Elle commence à 0 (très loin à gauche) et monte par "sauts" à chaque valeur possible de la variable, pour finalement atteindre 1 (très loin à droite). La hauteur de chaque saut correspond à la valeur de la PMF à ce point.
\end{intuitionbox}

\begin{examplebox}
Reprenons le lancer d'un dé équilibré ($X$). Calculons quelques valeurs de la CDF, notée $F(x)$.
\newline
$F(0.5) = P(X \le 0.5) = 0$
\newline
$F(1) = P(X \le 1) = P(X=1) = 1/6$
\newline
$F(1.5) = P(X \le 1.5) = P(X=1) = 1/6$
\newline
$F(2) = P(X \le 2) = P(X=1) + P(X=2) = 2/6$
\newline
$F(5.9) = P(X \le 5.9) = P(X=1) + \dots + P(X=5) = 5/6$
\newline
$F(6) = P(X \le 6) = 1$
\newline
$F(100) = P(X \le 100) = 1$
\end{examplebox}

\subsection{Variable Aléatoire Indicatrice}

\begin{definitionbox}[Variable Aléatoire Indicatrice]
La variable aléatoire indicatrice d'un événement $A$ est la variable aléatoire qui vaut 1 si $A$ se produit et 0 sinon. Nous la noterons $I_A$. Notez que $I_A \sim \text{Bern}(p)$ avec $p=P(A)$.
\end{definitionbox}

\begin{intuitionbox}
Une variable indicatrice est un interrupteur. Elle est sur "ON" (valeur 1) si un événement qui nous intéresse se produit, et sur "OFF" (valeur 0) sinon. C'est un outil extrêmement puissant car il transforme les questions sur les probabilités des événements en questions sur les espérances des variables aléatoires, ce qui simplifie souvent les calculs.
\end{intuitionbox}