\newpage

\section{Les Lois des Grands Nombres (LLN)}

Dans la section précédente, nous avons fait une distinction cruciale entre les \textbf{moments de population} (les "vraies" valeurs théoriques, inconnues, comme $\mu$ et $\sigma^2$) et les \textbf{moments d'échantillon} (nos estimations calculées à partir des données, comme $\bar{X}$ et $s^2$).

Par exemple, nous avons défini la moyenne d'échantillon $\bar{X} = \frac{1}{n} \sum X_i$ comme notre "meilleure estimation" de la moyenne de population $\mu$. Mais qu'est-ce qui nous garantit que cette estimation est "bonne" ? Qu'est-ce qui nous assure que si nous collections plus de données (en augmentant $n$), notre $\bar{X}$ se rapprocherait de $\mu$ ?

La réponse à cette question fondamentale est fournie par les \textbf{Lois des Grands Nombres (LLN)}. Elles forment le pont théorique entre les probabilités (la théorie) et les statistiques (la pratique).

\begin{intuitionbox}[L'Idée Fondamentale : L'Exemple du Dé]
Supposons que nous voulons connaître la valeur moyenne d'un lancer de dé équilibré.
\begin{itemize}
    \item \textbf{Moment de Population :} Nous savons par la théorie que $\mu = E[X] = \frac{1+2+3+4+5+6}{6} = 3.5$.
    
    \item \textbf{Moments d'Échantillon :} Nous n'avons pas cette information, alors nous lançons le dé.
    \begin{itemize}
        \item $n=2$ lancers : On obtient (2, 6). $\bar{X}_2 = (2+6)/2 = 4.0$. (Assez loin de 3.5)
        \item $n=10$ lancers : On obtient (1, 6, 3, 3, 5, 2, 4, 1, 6, 4). $\bar{X}_{10} = 3.5$. (Pile dessus !)
        \item $n=100$ lancers : On obtiendra $\bar{X}_{100} \approx 3.48$ (par exemple).
        \item $n=1,000,000$ lancers : On obtiendra $\bar{X}_{1,000,000} \approx 3.5001$ (par exemple).
    \end{itemize}
\end{itemize}
La Loi des Grands Nombres formalise cette intuition : à mesure que $n \to \infty$, la moyenne de notre échantillon $\bar{X}_n$ \textbf{converge} vers la vraie moyenne $\mu$.

La distinction entre les lois "Faible" et "Forte" réside dans la \textit{manière} dont nous définissons cette convergence.
\end{intuitionbox}


\subsection{L'Inégalité de Chebyshev}

Avant de prouver la Loi Faible, nous avons besoin d'un outil fondamental qui relie la variance d'une variable à la probabilité qu'elle s'éloigne de sa moyenne. C'est l'Inégalité de Chebyshev.

Sa puissance réside dans son universalité : elle s'applique à \textit{n'importe quelle} distribution, à condition qu'elle ait une moyenne et une variance finies.

\begin{theorembox}[Inégalité de Chebyshev]
Soit $Y$ une variable aléatoire avec une espérance finie $\mu = E[Y]$ et une variance finie $\sigma^2 = \text{Var}(Y)$.

Alors, pour tout nombre réel $k > 0$ :
$$ P(|Y - \mu| \ge k) \le \frac{\text{Var}(Y)}{k^2} = \frac{\sigma^2}{k^2} $$
\end{theorembox}

% --- DÉBUT DE L'AJOUT DE LA PREUVE ---
\begin{proofbox}[Preuve de l'Inégalité de Chebyshev]
Nous présentons la preuve pour une variable aléatoire continue $Y$ de densité $f(y)$. La preuve pour le cas discret est similaire en remplaçant les intégrales par des sommes.

\begin{enumerate}
    \item Par définition, la variance $\sigma^2$ est $E[(Y - \mu)^2]$ :
    $$ \sigma^2 = E[(Y - \mu)^2] = \int_{-\infty}^{\infty} (y - \mu)^2 f(y) dy $$
    
    \item Nous pouvons scinder cette intégrale en deux parties : la région où $Y$ est proche de $\mu$ ($|y - \mu| < k$) et la région où $Y$ est loin de $\mu$ ($|y - \mu| \ge k$) :
    $$ \sigma^2 = \int_{|y - \mu| < k} (y - \mu)^2 f(y) dy + \int_{|y - \mu| \ge k} (y - \mu)^2 f(y) dy $$
    
    \item L'intégrande $(y - \mu)^2 f(y)$ est toujours non-négative (un carré fois une densité). Par conséquent, la première intégrale est $\ge 0$. En la supprimant, nous ne pouvons que diminuer la valeur totale :
    $$ \sigma^2 \ge \int_{|y - \mu| \ge k} (y - \mu)^2 f(y) dy $$
    
    \item Maintenant, concentrons-nous sur la région d'intégration : $|y - \mu| \ge k$. Dans cette région, par définition, nous avons $(y - \mu)^2 \ge k^2$.
    
    \item Nous pouvons remplacer $(y - \mu)^2$ par $k^2$ dans l'intégrale. Puisque nous remplaçons un terme par quelque chose de plus petit ou égal, la valeur de l'intégrale diminue (ou reste égale) :
    $$ \sigma^2 \ge \int_{|y - \mu| \ge k} k^2 f(y) dy $$
    
    \item $k^2$ est une constante, nous pouvons la sortir de l'intégrale :
    $$ \sigma^2 \ge k^2 \int_{|y - \mu| \ge k} f(y) dy $$
    
    \item Par définition, l'intégrale de la densité $f(y)$ sur la région $|y - \mu| \ge k$ n'est autre que la probabilité $P(|Y - \mu| \ge k)$.
    $$ \sigma^2 \ge k^2 \cdot P(|Y - \mu| \ge k) $$
    
    \item En réarrangeant les termes (puisque $k > 0$, $k^2 > 0$), nous obtenons l'inégalité désirée :
    $$ P(|Y - \mu| \ge k) \le \frac{\sigma^2}{k^2} $$
\end{enumerate}
Cette preuve est un cas particulier de l'Inégalité de Markov (appliquée à la variable aléatoire non-négative $X = (Y-\mu)^2$ et à la constante $a = k^2$).
\end{proofbox}
% --- FIN DE L'AJOUT DE LA PREUVE ---

\begin{intuitionbox}[Comprendre l'Inégalité de Chebyshev]
Cette formule peut être lue comme suit :

\textbf{"La probabilité de s'écarter de la moyenne ($\mu$) d'au moins $k$ est bornée par la variance divisée par $k^2$."}

\begin{itemize}
    \item \textbf{Le rôle de la variance ($\sigma^2$) :} Si la variance est grande, la borne supérieure est élevée. L'inégalité nous dit "il est possible que la variable s'éloigne", ce qui est logique pour une grande dispersion. Si la variance est faible, la borne est basse, ce qui force la probabilité d'être loin à être faible.
    \item \textbf{Le rôle de l'écart ($k$) :} Le terme $k^2$ au dénominateur est crucial. Il signifie que la probabilité de s'écarter de la moyenne diminue \textit{quadratiquement} avec la distance $k$. Être très loin est (relativement) très improbable.
\end{itemize}
\end{intuitionbox}

\begin{examplebox}[Une Borne Universelle]
Exprimons l'inégalité en termes d'écarts-types (en posant $k = c \cdot \sigma$) :
$$ P(|Y - \mu| \ge c\sigma) \le \frac{\sigma^2}{(c\sigma)^2} = \frac{1}{c^2} $$

\begin{itemize}
    \item \textbf{Pour $c=2$ :} $P(|Y - \mu| \ge 2\sigma) \le \frac{1}{4} = 25\%$.
    Peu importe la distribution (symétrique, asymétrique, bizarre...), la probabilité d'être à 2 écarts-types ou plus de la moyenne est \textbf{au maximum} de 25\%. (Pour une loi normale, cette probabilité est bien plus faible, $\approx 4.55\%$).
    
    \item \textbf{Pour $c=3$ :} $P(|Y - \mu| \ge 3\sigma) \le \frac{1}{9} \approx 11.1\%$.
    La probabilité d'être à 3 écarts-types ou plus est au maximum de 11.1\%. (Pour une loi normale, c'est $\approx 0.27\%$).
\end{itemize}
Chebyshev fournit une borne "garantie", bien que souvent non optimale. Elle est l'outil parfait pour la preuve qui suit.
\end{examplebox}


\subsection{La Loi Faible des Grands Nombres (LFGN / WLLN)}

La loi faible stipule que la probabilité que notre moyenne d'échantillon s'écarte de la vraie moyenne de plus qu'une petite quantité $\epsilon$ tend vers zéro. C'est une \textbf{convergence en probabilité}.

\begin{definitionbox}[Convergence en Probabilité]
On dit qu'une suite de variables aléatoires $Y_n$ converge en probabilité vers une constante $c$, noté $Y_n \xrightarrow{P} c$, si pour tout $\epsilon > 0$ (aussi petit soit-il) :
$$\lim_{n \to \infty} P(|Y_n - c| > \epsilon) = 0$$
\end{definitionbox}

\begin{intuitionbox}[Comprendre la Convergence en Probabilité]
La définition $P(|\bar{X}_n - \mu| > \epsilon) \to 0$ signifie :
\begin{itemize}
    \item $\epsilon$ est votre \textbf{marge d'erreur} acceptable (ex: 0.01).
    \item $|\bar{X}_n - \mu|$ est l'erreur réelle de votre estimation.
    \item $P(\dots)$ est la probabilité que votre erreur \textbf{dépasse} votre marge.
    \item $\lim_{n \to \infty} (\dots) = 0$ signifie : "Si vous prenez un échantillon $n$ suffisamment grand, la probabilité de faire une erreur plus grande que $\epsilon$ devient négligeable."
\end{itemize}
C'est une affirmation sur ce qui se passe pour un $n$ fixe et très grand.
\end{intuitionbox}

\begin{theorembox}[Loi Faible des Grands Nombres (Khinchine)]
Soit $X_1, X_2, \dots, X_n$ une suite de variables aléatoires \textbf{i.i.d.} (indépendantes et identiquement distribuées) avec une espérance finie $E[X_i] = \mu$.
Soit $\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$ la moyenne d'échantillon.

Alors, $\bar{X}_n$ converge en probabilité vers $\mu$ :
$$\bar{X}_n \xrightarrow{P} \mu$$
\end{theorembox}

\begin{proofbox}[Preuve (simplifiée) via l'Inégalité de Chebyshev]
La loi faible de Khinchine ne nécessite qu'une moyenne finie. Cependant, si nous ajoutons la condition que la \textbf{variance $\sigma^2$ est aussi finie}, la preuve devient très simple.

Elle repose directement sur l'Inégalité de Chebyshev, que nous venons de voir. Nous l'appliquons à la variable aléatoire $Y = \bar{X}_n$.

\begin{enumerate}
    \item Identifions les termes pour l'inégalité $P(|Y - E[Y]| \ge k) \le \frac{\text{Var}(Y)}{k^2}$:
    \begin{itemize}
        \item Notre variable est $Y = \bar{X}_n$.
        \item Son espérance est $E[Y] = E[\bar{X}_n] = \mu$.
        \item Sa variance est $\text{Var}(Y) = \text{Var}(\bar{X}_n) = \frac{\sigma^2}{n}$.
        \item Notre écart $k$ est la marge d'erreur $\epsilon$.
    \end{itemize}

    \item (Rappel du calcul de la variance de $\bar{X}_n$) :
    Puisque les $X_i$ sont i.i.d., $\text{Var}(\bar{X}_n) = \text{Var}\left(\frac{1}{n}\sum X_i\right) = \frac{1}{n^2}\sum \text{Var}(X_i) = \frac{1}{n^2}(n\sigma^2) = \frac{\sigma^2}{n}$.

    \item Appliquons l'inégalité de Chebyshev avec ces termes :
    $$ P(|\bar{X}_n - \mu| \ge \epsilon) \le \frac{\text{Var}(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2 / n}{\epsilon^2} = \frac{\sigma^2}{n \epsilon^2} $$
    
    \item Prenons maintenant la limite quand $n \to \infty$ :
    $$ \lim_{n \to \infty} P(|\bar{X}_n - \mu| \ge \epsilon) \le \lim_{n \to \infty} \frac{\sigma^2}{n \epsilon^2} $$
    
    \item Puisque $\sigma^2$ et $\epsilon^2$ sont des constantes finies, le terme de droite $\frac{\text{constante}}{n}$ tend vers 0.
    
    \item Comme une probabilité ne peut pas être négative, nous avons :
    $$ \lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0 $$
\end{enumerate}
C'est exactement la définition de la convergence en probabilité.
\end{proofbox}

\subsection{La Loi Forte des Grands Nombres (LFGN / SLLN)}

La loi forte est une affirmation beaucoup plus puissante. Elle ne dit pas seulement qu'un "gros" écart est improbable pour un $n$ "grand" ; elle dit que la probabilité que la suite $\bar{X}_n$ \textit{ne converge pas} vers $\mu$ est nulle. C'est une \textbf{convergence presque sûre}.

\begin{definitionbox}[Convergence Presque Sûre]
On dit qu'une suite de variables aléatoires $Y_n$ converge presque sûrement vers une constante $c$, noté $Y_n \xrightarrow{p.s.} c$, si :
$$P\left( \lim_{n \to \infty} Y_n = c \right) = 1$$
\end{definitionbox}

\begin{theorembox}[Loi Forte des Grands Nombres (Kolmogorov)]
Soit $X_1, X_2, \dots, X_n$ une suite de variables aléatoires \textbf{i.i.d.} avec une espérance finie $E[X_i] = \mu$.
Alors, $\bar{X}_n$ converge presque sûrement vers $\mu$ :
$$\bar{X}_n \xrightarrow{p.s.} \mu$$
\end{theorembox}

\begin{remarquebox}[Forte implique Faible]
La convergence "presque sûre" (SLLN) est une condition plus stricte que la convergence "en probabilité" (WLLN). Si une suite converge presque sûrement, elle converge aussi en probabilité. L'inverse n'est pas toujours vrai.
\end{remarquebox}

\subsection{Différence : Faible vs. Forte}

\begin{intuitionbox}[Faible vs. Forte : L'Analogie du Casino]
Soit $\bar{X}_n$ votre gain moyen par partie après avoir joué $n$ fois à la roulette. La vraie moyenne (l'avantage de la maison) est $\mu = -0.053$ (pour une roulette américaine).

\begin{itemize}
    \item \textbf{Loi Faible (WLLN) :} "Si vous prévoyez de jouer $n = 1 \text{ million}$ de parties ce soir. La probabilité qu'à la fin de votre millionième partie, votre moyenne $\bar{X}_{1,000,000}$ soit loin de $-0.053$ (par exemple, que vous soyez gagnant, $\bar{X}_n > 0$) est infinitésimale."
    \item C'est une affirmation sur la distribution de $\bar{X}_n$ \textbf{à un point fixe $n$ (très grand)}. Elle n'exclut pas la possibilité théorique (mais improbable) que si vous continuiez à jouer, votre moyenne $\bar{X}_n$ puisse à nouveau diverger follement avant de reconverger plus tard.

    \item \textbf{Loi Forte (SLLN) :} "Si vous jouez à la roulette \textit{pour l'éternité}, en regardant la séquence de vos moyennes $\bar{X}_1, \bar{X}_2, \bar{X}_3, \dots, \bar{X}_n, \dots$."
    \item "La probabilité que cette \textbf{séquence entière} ne converge pas exactement vers $\mu = -0.053$ est de 0."
    \item C'est une affirmation sur la \textbf{trajectoire complète}. Elle dit que, avec une probabilité de 1, la trajectoire de $\bar{X}_n$ va s'approcher de $\mu$ et \textbf{ne plus s'en écarter} de manière significative.
\end{itemize}

En résumé :
\begin{itemize}
    \item \textbf{Faible :} Pour $n$ assez grand, un écart est \textbf{improbable}.
    \item \textbf{Forte :} La \textbf{trajectoire} converge vers $\mu$ (avec une probabilité de 1).
\end{itemize}
\end{intuitionbox}

\subsection{Application : La Méthode de Monte-Carlo}

La Loi Forte des Grands Nombres est le moteur de l'une des techniques de calcul les plus puissantes : la simulation de Monte-Carlo. Elle nous permet d'estimer des quantités complexes (comme des intégrales) en utilisant le hasard.

\begin{examplebox}[Estimer la valeur de $\pi$]

\textbf{Problème :} Comment calculer $\pi$ sans formule géométrique ?

\textbf{Méthode (Statistique) :}
\begin{enumerate}
    \item Imaginez un carré de côté 1 (de $(0,0)$ à $(1,1)$). Son aire est $A_{\text{carré}} = 1$.
    \item Imaginez un quart de cercle de rayon $r=1$ inscrit dans ce carré. Son aire est $A_{\text{cercle}} = \frac{1}{4}\pi r^2 = \frac{\pi}{4}$.
    \item Le \textit{ratio} des aires est $\frac{A_{\text{cercle}}}{A_{\text{carré}}} = \frac{\pi / 4}{1} = \frac{\pi}{4}$.
\end{enumerate}

\textbf{Simulation :}
\begin{enumerate}
    \item Nous allons "lancer des fléchettes" au hasard sur ce carré $n$ fois.
    \item Pour ce faire, nous générons $n$ paires de nombres aléatoires $(X_i, Y_i)$, où $X_i \sim U(0, 1)$ et $Y_i \sim U(0, 1)$.
    \item Pour chaque point $i$, nous vérifions s'il a atterri \textbf{dans le cercle}. La condition est $X_i^2 + Y_i^2 \le 1$.
    \item Nous définissons une nouvelle variable aléatoire $Z_i$ (de Bernoulli) :
$$ Z_i = \begin{cases} 1 & \text{si } X_i^2 + Y_i^2 \le 1 \quad \text{(le point est dans le cercle)} \\ 0 & \text{sinon} \end{cases} $$
\end{enumerate}

\textbf{Application de la LLN :}
\begin{itemize}
    \item Quelle est la "vraie moyenne" $\mu$ de cette variable $Z_i$ ?
    \item $\mu = E[Z_i] = 1 \cdot P(Z_i=1) + 0 \cdot P(Z_i=0) = P(Z_i=1)$.
    \item $P(Z_i=1)$ est la probabilité qu'un point aléatoire tombe dans le cercle. Puisque les points sont uniformes, cette probabilité est simplement le ratio des aires !
    \item Donc, la vraie moyenne (inconnue) est $\mu = \frac{A_{\text{cercle}}}{A_{\text{carré}}} = \frac{\pi}{4}$.
    
    \item Comment estimer $\mu$ ? Nous utilisons la moyenne d'échantillon $\bar{Z}_n$ :
    $$\bar{Z}_n = \frac{1}{n} \sum_{i=1}^n Z_i = \frac{\text{Nombre de points dans le cercle}}{n}$$
    
    \item Par la \textbf{Loi Forte des Grands Nombres}, nous avons la garantie que :
    $$\bar{Z}_n \xrightarrow{p.s.} \mu = \frac{\pi}{4}$$
\end{itemize}

\textbf{Conclusion :}
Pour estimer $\pi$, il suffit de calculer $\bar{Z}_n$ (une simple proportion) et de la multiplier par 4.
$$\pi \approx 4 \cdot \bar{Z}_n$$
Plus notre nombre de simulations $n$ est grand, plus la SLLN nous garantit que notre estimation sera proche de la vraie valeur de $\pi$.
\end{examplebox}