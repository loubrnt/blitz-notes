\newpage
\section{Variables Aléatoires Discrètes}

\subsection{Variable Aléatoire}

Jusqu'à présent, nous avons parlé d'événements (comme "obtenir Pile" ou "tirer un Roi"). Pour analyser ces phénomènes avec des outils mathématiques plus puissants, nous devons traduire ces résultats concrets en nombres. C'est le rôle de la variable aléatoire.

\begin{definitionbox}[Variable Aléatoire]
Étant donné une expérience avec un univers $S$, une variable aléatoire est une fonction de l'univers $S$ vers les nombres réels $\mathbb{R}$.
\end{definitionbox}

Cette définition formelle masque une idée très simple :

\begin{intuitionbox}
Une variable aléatoire est une manière de traduire les résultats d'une expérience en nombres. Au lieu de travailler avec des concepts comme "Pile" ou "Face", on leur assigne des valeurs numériques (par exemple, 1 pour Pile, 0 pour Face). Cela nous permet d'utiliser toute la puissance des outils mathématiques (fonctions, calculs, etc.) pour analyser le hasard. C'est un pont entre le monde concret des événements et le monde abstrait des nombres.
\end{intuitionbox}

Prenons un exemple classique :

\begin{examplebox}
On lance deux dés. L'univers $S$ est l'ensemble des 36 paires de résultats, comme $(1,1), (1,2), \dots, (6,6)$. On peut définir une variable aléatoire $X$ comme étant la \textbf{somme des deux dés}.
Pour le résultat $(2, 5)$, la valeur de la variable aléatoire est $X(2, 5) = 2 + 5 = 7$.
\end{examplebox}

\subsection{Variable Aléatoire Discrète}

Les variables aléatoires peuvent être de différents types. Nous commençons par le type le plus simple à "compter".

\begin{definitionbox}[Variable Aléatoire Discrète]
Une variable aléatoire $X$ est dite discrète s'il existe une liste finie ou infinie dénombrable de valeurs $a_1, a_2, \dots$ telle que $P(X=a_j \text{ pour un certain } j) = 1$.
\end{definitionbox}

L'analogie la plus simple pour comprendre le terme "discret" est celle d'un escalier.

\begin{intuitionbox}
Une variable aléatoire est "discrète" si on peut lister (compter) toutes les valeurs qu'elle peut prendre, même si cette liste est infinie. Pensez aux "sauts" d'une valeur à l'autre, sans possibilité de prendre une valeur intermédiaire. C'est comme monter un escalier : on peut être sur la marche 1, 2 ou 3, mais jamais sur la marche 2.5. Le nombre de têtes en 10 lancers, le résultat d'un dé, le nombre d'emails que vous recevez en une heure sont des exemples. À l'opposé, une variable continue pourrait être la taille exacte d'une personne, qui peut prendre n'importe quelle valeur dans un intervalle.
\end{intuitionbox}

\subsection{Fonction de Masse (PMF)}

Maintenant que nous avons une variable aléatoire qui produit des nombres discrets, nous avons besoin d'une fonction pour décrire la probabilité de chacun de ces nombres.

\begin{definitionbox}[Probability Mass Function (PMF)]
La fonction de masse (PMF) d'une variable aléatoire discrète $X$ est la fonction $P_X$ donnée par $P_X(x) = P(X=x)$.
\end{definitionbox}

C'est la "carte d'identité" probabiliste de la variable :

\begin{intuitionbox}
La PMF est la "carte d'identité" probabiliste d'une variable aléatoire discrète. Pour chaque valeur que la variable peut prendre, la PMF nous donne la probabilité exacte associée à cette valeur. C'est comme si chaque résultat possible avait une "étiquette de prix" qui indique sa chance de se produire. La somme de toutes ces probabilités doit bien sûr valoir 1.
\end{intuitionbox}

Un exemple très simple est le lancer de dé :

\begin{examplebox}
Soit $X$ le résultat d'un lancer de dé équilibré. La variable $X$ peut prendre les valeurs $\{1, 2, 3, 4, 5, 6\}$.
La PMF de $X$ est la fonction qui assigne $1/6$ à chaque valeur :
$P(X=1) = 1/6$, $P(X=2) = 1/6$, ..., $P(X=6) = 1/6$.
Pour toute autre valeur $x$ (par exemple $x=2.5$ ou $x=7$), $P(X=x) = 0$.
\end{examplebox}

\subsection{Loi de Bernoulli}

Commençons par la loi de probabilité discrète la plus simple.

\begin{definitionbox}[Distribution de Bernoulli]
Une variable aléatoire $X$ suit la distribution de Bernoulli avec paramètre $p$ si $P(X=1) = p$ et $P(X=0) = 1-p$, où $0 < p < 1$. On note cela $X \sim \text{Bern}(p)$.
\end{definitionbox}

C'est la brique fondamentale de nombreuses autres distributions.

\begin{intuitionbox}
La distribution de Bernoulli est le modèle le plus simple pour une expérience aléatoire avec seulement deux issues : "succès" (codé par 1) et "échec" (codé par 0). C'est la brique de base de nombreuses autres distributions. Pensez à un unique lancer de pièce (Pile/Face), un unique tir au but (Marqué/Manqué), ou la réponse à une question par oui/non. Le paramètre $p$ est simplement la probabilité du "succès".
\end{intuitionbox}

\subsection{Loi Binomiale}

Que se passe-t-il si nous répétons une expérience de Bernoulli $n$ fois et que nous comptons le nombre total de succès ?

\begin{theorembox}[PMF Binomiale]
Si $X \sim \text{Bin}(n, p)$, alors la PMF de $X$ est :
$$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
pour $k = 0, 1, \dots, n$.
\end{theorembox}

La preuve de cette formule est un argument combinatoire direct.

\begin{proofbox}
Nous voulons trouver la probabilité d'obtenir exactement $k$ succès au cours de $n$ essais indépendants.
\begin{enumerate}
    \item \textbf{Probabilité d'une séquence spécifique :} Considérons d'abord une séquence spécifique contenant $k$ succès (S) et $n-k$ échecs (E), par exemple $S, S, \dots, S, E, E, \dots, E$.
    Puisque les essais sont indépendants, la probabilité de cette séquence est le produit des probabilités individuelles :
    $$ \underbrace{p \times p \times \dots \times p}_{k \text{ fois}} \times \underbrace{(1-p) \times \dots \times (1-p)}_{n-k \text{ fois}} = p^k (1-p)^{n-k} $$
    
    \item \textbf{Nombre de séquences possibles :} La séquence ci-dessus n'est qu'une des nombreuses façons d'obtenir $k$ succès. Le nombre total de façons d'arranger $k$ succès parmi $n$ positions (essais) est donné par le coefficient binomial $\binom{n}{k}$.
    
    \item \textbf{Probabilité totale :} Chacune de ces $\binom{n}{k}$ séquences a la même probabilité $p^k (1-p)^{n-k}$. Puisque toutes ces séquences sont des événements disjoints, la probabilité totale d'obtenir $k$ succès (dans n'importe quel ordre) est la somme de leurs probabilités :
    $$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
\end{enumerate}
\end{proofbox}

Chaque partie de cette formule a une signification logique claire.

\begin{intuitionbox}
La distribution binomiale répond à la question : "Si je répète $n$ fois la même expérience de Bernoulli (qui a une probabilité de succès $p$), quelle est la probabilité d'obtenir exactement $k$ succès ?"
La formule est construite logiquement en multipliant trois composantes. D'abord, $\mathbf{p^k}$ représente la probabilité d'obtenir $k$ succès. Ensuite, $\mathbf{(1-p)^{n-k}}$ est la probabilité que les $n-k$ échecs restants se produisent. Finalement, comme les $k$ succès peuvent apparaître n'importe où parmi les $n$ essais, on multiplie par $\mathbf{\binom{n}{k}}$, qui compte le nombre de manières distinctes de placer ces succès.
\end{intuitionbox}

Appliquons cela à un exemple classique :

\begin{examplebox}
On lance une pièce équilibrée 10 fois ($n=10$, $p=0.5$). Quelle est la probabilité d'obtenir exactement 6 Piles ($k=6$) ?
$$ P(X=6) = \binom{10}{6} (0.5)^6 (1-0.5)^{10-6} = \frac{10!}{6!4!} (0.5)^{10} = 210 \times (0.5)^{10} \approx 0.205 $$
Il y a environ 20.5\% de chance d'obtenir exactement 6 Piles.
\end{examplebox}

\subsection{Loi Hypergéométrique}

La loi binomiale suppose que les essais sont indépendants, ce qui est vrai si l'on tire *avec remise*. Que se passe-t-il si l'on tire *sans remise* ?

\begin{theorembox}[PMF Hypergéométrique]
Si $X \sim \text{HG}(w, b, m)$, alors la PMF de $X$ est :
$$ P(X=k) = \frac{\binom{w}{k} \binom{b}{m-k}}{\binom{w+b}{m}} $$
\end{theorembox}

La preuve de cette formule est un argument de dénombrement pur, basé sur la définition naïve de la probabilité.

\begin{proofbox}
Nous utilisons la définition naïve $P(A) = |A| / |S|$.
Nous tirons $m$ boules d'une urne contenant $w$ blanches et $b$ noires, soit $w+b$ boules au total.

\begin{enumerate}
    \item \textbf{Taille de l'univers ($|S|$)} : Le nombre total de façons de choisir $m$ boules parmi $w+b$ est $\binom{w+b}{m}$.
    
    \item \textbf{Taille de l'événement favorable ($|A|$)} : Nous voulons l'événement $A$ = "obtenir exactement $k$ boules blanches ET $m-k$ boules noires".
    \begin{itemize}
        \item Le nombre de façons de choisir $k$ blanches parmi $w$ est $\binom{w}{k}$.
        \item Le nombre de façons de choisir $m-k$ noires parmi $b$ est $\binom{b}{m-k}$.
    \end{itemize}
    Par le principe de la multiplication (dénombrement), le nombre total de façons de réaliser $A$ est $|A| = \binom{w}{k} \binom{b}{m-k}$.
    
    \item \textbf{Probabilité :} En divisant le nombre d'issues favorables par le nombre total d'issues, on obtient :
    $$ P(X=k) = \frac{|A|}{|S|} = \frac{\binom{w}{k} \binom{b}{m-k}}{\binom{w+b}{m}} $$
\end{enumerate}
\end{proofbox}

Chaque terme de cette fraction a un sens très concret :

\begin{intuitionbox}
La distribution hypergéométrique est la "cousine" de la binomiale pour les tirages \textbf{sans remise}. Imaginez une urne avec des boules de deux couleurs (par exemple, $w$ blanches et $b$ noires). Vous tirez $m$ boules d'un coup. Quelle est la probabilité que vous ayez exactement $k$ boules blanches ?
La formule est un simple ratio issu du dénombrement. Le \textbf{dénominateur}, $\binom{w+b}{m}$, compte le nombre total de façons de tirer $m$ boules parmi toutes celles disponibles. Le \textbf{numérateur} compte les issues favorables : c'est le produit du nombre de façons de choisir $k$ blanches parmi les $w$ ($\binom{w}{k}$) ET de choisir les $m-k$ boules restantes parmi les noires ($\binom{b}{m-k}$). La différence clé avec la loi binomiale est que les tirages ne sont pas indépendants.
\end{intuitionbox}

Un exemple typique est la formation de comités à partir d'un groupe.

\begin{examplebox}
Un comité de 5 personnes est choisi au hasard parmi un groupe de 8 hommes et 10 femmes. Quelle est la probabilité que le comité soit composé de 2 hommes et 3 femmes ?
Ici, on tire 5 personnes ($m=5$) d'une population de 18 personnes. On s'intéresse au nombre d'hommes ($k=2$) parmi les 8 disponibles ($w=8$). Le reste du comité sera composé de femmes ($b=10$).
$$ P(X=2) = \frac{\binom{8}{2} \binom{10}{3}}{\binom{18}{5}} = \frac{28 \times 120}{8568} \approx 0.392 $$
Il y a environ 39.2\% de chance que le comité ait exactement cette composition.
\end{examplebox}

\subsection{Loi Géométrique}

Revenons aux essais de Bernoulli (indépendants). Au lieu de fixer le nombre d'essais $n$, demandons-nous : combien d'essais faut-il avant d'obtenir notre premier succès ?

\begin{theorembox}[PMF de la loi géométrique]
Une variable aléatoire $X$ suit la loi géométrique de paramètre $p$, notée $X \sim \text{Geom}(p)$, si elle modélise le nombre d'échecs avant le premier succès dans une série d'épreuves de Bernoulli indépendantes. Sa fonction de masse (PMF) est :
$$ P(X=k) = (1-p)^k p \quad \text{pour } k=0, 1, 2, \dots $$
où $q = 1-p$ est la probabilité d'échec.
\end{theorembox}

La preuve de cette formule est une application directe de l'indépendance des essais.

\begin{proofbox}
Soit $S_i$ l'événement "succès au $i$-ème essai" et $E_i$ l'événement "échec au $i$-ème essai".
L'événement $\{X=k\}$ signifie que nous avons observé exactement $k$ échecs, suivis d'un succès au $(k+1)$-ème essai.
C'est la séquence d'événements : $E_1 \cap E_2 \cap \dots \cap E_k \cap S_{k+1}$.

Puisque tous les essais sont indépendants, la probabilité de cette intersection est le produit des probabilités individuelles :
\begin{align*}
P(X=k) &= P(E_1) \times P(E_2) \times \dots \times P(E_k) \times P(S_{k+1}) \\
&= \underbrace{(1-p) \times (1-p) \times \dots \times (1-p)}_{k \text{ fois}} \times p \\
&= (1-p)^k p
\end{align*}
\end{proofbox}

La formule est donc très littérale :

\begin{intuitionbox}
La formule $P(X=k) = q^k p$ décrit la probabilité d'une séquence très spécifique : $k$ échecs consécutifs (chacun avec une probabilité $q$, donc $q^k$ pour la série), suivis immédiatement d'un succès (avec une probabilité $p$). C'est la loi de "l'attente du premier succès".
\end{intuitionbox}

Un exemple classique est l'attente d'un résultat spécifique sur un dé.

\begin{examplebox}[Premier 6 au lancer de dé]
On lance un dé jusqu'à obtenir un 6. La probabilité de succès est $p=1/6$, et celle d'échec est $q=5/6$. Quelle est la probabilité que l'on ait besoin de 3 lancers (donc 2 échecs avant le premier succès) ?
Ici, $k=2$. La probabilité est :
$$ P(X=2) = (5/6)^2 \cdot (1/6) = \frac{25}{216} \approx 0.116 $$
\end{examplebox}

\subsection{Loi de Poisson}

Introduisons maintenant une loi utilisée pour modéliser le nombre d'événements se produisant dans un intervalle de temps ou d'espace fixe.

\begin{definitionbox}[Distribution de Poisson]
Une variable aléatoire $X$ suit la loi de Poisson de paramètre $\lambda > 0$ si sa PMF est donnée par :
$$ P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!} \quad \text{pour } k=0, 1, 2, \dots $$
Elle modélise typiquement le nombre d'événements se produisant dans un intervalle de temps ou d'espace fixe.
\end{definitionbox}

Cette loi est souvent appelée la loi des événements rares.

\begin{intuitionbox}
La loi de Poisson est la loi des événements rares. Imaginez que vous comptez le nombre d'appels arrivant à un standard téléphonique en une minute. Il y a de nombreux instants où un appel pourrait arriver, mais la probabilité à chaque instant est infime. La loi de Poisson modélise ce type de scénario, où l'on connaît seulement le taux moyen d'arrivée des événements ($\lambda$).
\end{intuitionbox}

Mais d'où vient cette formule avec $e$ et une factorielle ? Elle vient d'une approximation de la loi binomiale lorsque $n$ est très grand et $p$ très petit.

\begin{theorembox}[La loi de Poisson comme limite de la loi binomiale]
Soit $X_n \sim \text{Bin}(n, p_n)$, où $\lambda = np_n$ est une constante positive fixée. Alors, pour tout $k \in \{0, 1, 2, \dots\}$, nous avons :
$$ \lim_{n \to \infty} P(X_n=k) = \frac{e^{-\lambda}\lambda^k}{k!} $$
En pratique, la loi de Poisson est une excellente approximation de la loi binomiale quand $n$ est grand et $p$ est petit.
\end{theorembox}

% --- INTUITION AMÉLIORÉE ---
\begin{intuitionbox}[Convergence Binomiale vers Poisson : L'Exemple des Naissances]
Supposons que les bébés naissent dans une grande ville à un taux moyen de $\lambda=10$ naissances par jour. Comment modéliser le nombre $X$ de naissances un jour donné ?

\textbf{1. Approche Binomiale (Découpage du Temps) :}
On peut diviser la journée (24h) en $n$ très petits intervalles de temps (par exemple, $n = 24 \times 60 \times 60 = 86400$ secondes).
\begin{itemize}
    \item Si $n$ est très grand, la chance $p$ qu'une naissance se produise \textit{exactement} pendant une seconde donnée est minuscule. On peut calculer cette probabilité $p$ comme le taux moyen divisé par le nombre d'intervalles : $p = \lambda / n = 10 / 86400$.
    \item On peut aussi supposer que la probabilité d'avoir *deux* naissances ou plus dans la même seconde est négligeable. Chaque seconde est donc comme un mini-essai de Bernoulli : soit 1 naissance (avec probabilité $p$), soit 0 naissance (avec probabilité $1-p$).
    \item Le nombre total de naissances $X$ sur la journée est la somme de ces $n$ essais de Bernoulli quasi-indépendants. $X$ suit donc approximativement une loi binomiale : $X \approx \text{Bin}(n, p=\lambda/n)$.
\end{itemize}
La probabilité d'avoir exactement $k$ naissances serait $P(X=k) \approx \binom{n}{k} p^k (1-p)^{n-k}$.

\textbf{2. Le Passage à la Limite (Modèle Continu) :}
Que se passe-t-il si on rend les intervalles de temps infiniment petits ($n \to \infty$) ? C'est là que la magie opère :
\begin{itemize}
    \item Le terme $\binom{n}{k}$ (combien de façons de choisir $k$ secondes parmi $n$) se comporte comme $n^k/k!$ pour $n$ grand.
    \item Le terme $p^k = (\lambda/n)^k$ devient $\lambda^k / n^k$.
    \item Le terme $(1-p)^{n-k} = (1-\lambda/n)^{n-k}$. Comme $k$ est petit par rapport à $n$, ceci est très proche de $(1-\lambda/n)^n$, qui tend vers $e^{-\lambda}$.
\end{itemize}
En combinant ces approximations (expliquées plus en détail dans la preuve formelle), on trouve que la probabilité $P(X=k)$ tend vers $\frac{n^k}{k!} \frac{\lambda^k}{n^k} e^{-\lambda} = \frac{e^{-\lambda}\lambda^k}{k!}$.

\textbf{Conclusion :}
La loi de Poisson apparaît naturellement comme la limite d'un processus binomial où l'on a un très grand nombre d'opportunités ($n$) pour qu'un événement rare (probabilité $p$) se produise, tout en maintenant un taux moyen constant ($\lambda = np$).
\end{intuitionbox}
% --- FIN INTUITION AMÉLIORÉE ---

La preuve formelle montre comment les termes de la formule binomiale se transforment en ceux de la formule de Poisson lorsque $n \to \infty$.

% --- PROOFBOX (IDENTIQUE À LA PRÉCÉDENTE) ---
\begin{proofbox}[Dérivation de la loi de Poisson à partir de la loi Binomiale (Détaillée)]
On part de la fonction de masse (PMF) d'une variable aléatoire $X_n$ suivant une loi binomiale $\text{Bin}(n, p)$, où l'on pose $p = \lambda/n$. L'objectif est de trouver la limite de cette PMF lorsque $n$ tend vers l'infini, tout en gardant $\lambda = np$ constant (ce qui implique que $p$ doit tendre vers 0).

La PMF binomiale est :
$$ P(X_n=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
Substituons $p = \lambda/n$ :
$$ P(X_n=k) = \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} $$
Maintenant, développons le coefficient binomial $\binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n(n-1)(n-2)\cdots(n-k+1)}{k!}$ :
$$ P(X_n=k) = \frac{n(n-1)\cdots(n-k+1)}{k!} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} $$
Réorganisons les termes pour isoler ceux qui dépendent de $n$ :
$$ P(X_n=k) = \frac{\lambda^k}{k!} \times \frac{n(n-1)\cdots(n-k+1)}{n^k} \times \left(1-\frac{\lambda}{n}\right)^n \times \left(1-\frac{\lambda}{n}\right)^{-k} $$
Nous allons maintenant examiner la limite de chaque partie lorsque $n \to \infty$, pour $k$ et $\lambda$ fixés.

\begin{enumerate}
    \item $\mathbf{\frac{\lambda^k}{k!}}$ : Ce terme est constant par rapport à $n$, donc sa limite est lui-même.
    
    \item $\mathbf{\frac{n(n-1)\cdots(n-k+1)}{n^k}}$ : Ce terme est un produit de $k$ facteurs divisé par $n^k$. On peut le réécrire comme :
    $$ \frac{n}{n} \times \frac{n-1}{n} \times \frac{n-2}{n} \times \cdots \times \frac{n-k+1}{n} $$
    $$ = 1 \times \left(1-\frac{1}{n}\right) \times \left(1-\frac{2}{n}\right) \times \cdots \times \left(1-\frac{k-1}{n}\right) $$
    Lorsque $n \to \infty$, chacun des termes $\frac{1}{n}, \frac{2}{n}, \dots, \frac{k-1}{n}$ tend vers 0 (car $k$ est fixe). Donc, chaque parenthèse tend vers $(1-0)=1$. Puisqu'il y a un nombre \textit{fixe} $k$ de termes dans le produit, la limite du produit est le produit des limites :
    $$ \lim_{n \to \infty} \frac{n(n-1)\cdots(n-k+1)}{n^k} = 1 \times 1 \times \cdots \times 1 = 1 $$
    \textit{Intuition : Pour $n$ très grand par rapport à $k$, les $k$ termes $n, n-1, \dots, n-k+1$ sont tous "presque" égaux à $n$. Leur produit est donc "presque" $n^k$, et le ratio est "presque" 1.}

    \item $\mathbf{\left(1-\frac{\lambda}{n}\right)^n}$ : C'est une limite fondamentale en analyse. On sait que pour tout réel $x$, $\lim_{n \to \infty} (1 + x/n)^n = e^x$. Ici, nous avons $x = -\lambda$. Donc :
    $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda} $$
    \textit{Intuition : C'est la définition même de l'exponentielle comme limite d'intérêts composés continus (ici, avec un taux négatif).}

    \item $\mathbf{\left(1-\frac{\lambda}{n}\right)^{-k}}$ : Lorsque $n \to \infty$, le terme $\lambda/n$ tend vers 0. L'expression à l'intérieur de la parenthèse tend donc vers $(1-0) = 1$. Puisque $k$ est un exposant fixe :
    $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = 1^{-k} = 1 $$
    \textit{Intuition : Pour $n$ très grand, $(1-\lambda/n)$ est très proche de 1. Élever ce nombre très proche de 1 à une puissance fixe $k$ le laisse très proche de 1.}
\end{enumerate}
Finalement, en multipliant les limites de chaque partie (puisque la limite d'un produit est le produit des limites) :
$$ \lim_{n \to \infty} P(X_n=k) = \left(\frac{\lambda^k}{k!}\right) \times (1) \times (e^{-\lambda}) \times (1) = \frac{e^{-\lambda}\lambda^k}{k!} $$
Ceci est exactement la fonction de masse de probabilité d'une loi de Poisson de paramètre $\lambda$.
\end{proofbox}
% --- FIN PROOFBOX ---


Un ensemble de données historiques célèbres illustre parfaitement cette loi.

\begin{examplebox}[Décès par ruade de cheval : Les données de Bortkiewicz]
En 1898, le statisticien Ladislaus Bortkiewicz a publié des données célèbres sur le nombre de soldats de la cavalerie prussienne tués par des ruades de cheval. Ces données sont un exemple classique d'application de la loi de Poisson pour modéliser des événements rares.

\noindent\textbf{Contexte et calcul du paramètre $\lambda$ :}
Sur une période de 20 ans, en observant 10 corps d'armée, il a collecté des données sur 200 "corps-années". Durant cette période, il y a eu un total de 122 décès. Le taux moyen de décès par corps-année est donc :
$$ \lambda = \frac{\text{Nombre total de décès}}{\text{Nombre total de corps-années}} = \frac{122}{200} = 0.61 $$
Le nombre de décès par corps-année, $X$, est donc modélisé par une loi de Poisson : $X \sim \text{Poisson}(\lambda=0.61)$.

\noindent\textbf{Comparaison des données observées et des prédictions du modèle :}
On peut calculer la probabilité d'observer $k$ décès en une année-corps en utilisant la PMF de Poisson : $P(X=k) = \frac{e^{-0.61} (0.61)^k}{k!}$. En multipliant cette probabilité par le nombre total d'observations (200), on obtient le nombre de cas attendus (nombre de corps d'armes dans lequels il y a k deces).

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Nombre de décès (k)} & \textbf{Observé} & \textbf{Probabilité de Poisson} & \textbf{Attendu} \\
\hline
0 & 109 & $P(X=0) \approx 0.543$ & 108.7 \\
1 & 65 & $P(X=1) \approx 0.331$ & 66.3 \\
2 & 22 & $P(X=2) \approx 0.101$ & 20.2 \\
3 & 3 & $P(X=3) \approx 0.021$ & 4.1 \\
4 & 1 & $P(X=4) \approx 0.003$ & 0.6 \\
5+ & 0 & $P(X \ge 5) \approx 0.000$ & 0.0 \\
\hline
\end{tabular}
\end{center}

L'adéquation remarquable entre les fréquences observées et les valeurs attendues par le modèle de Poisson a contribué à populariser cette distribution pour l'analyse d'événements rares.
\end{examplebox}

\subsection{Fonction de Répartition (CDF)}

Nous avons la PMF, qui donne $P(X=x)$. Une autre fonction tout aussi importante est la fonction de répartition (CDF), qui "accumule" ces probabilités.

\begin{definitionbox}[Cumulative Distribution Function (CDF)]
La fonction de répartition (CDF) d'une variable aléatoire $X$ est la fonction $F_X$ donnée par $F_X(x) = P(X \le x)$.
\end{definitionbox}

Cette fonction répond à une question différente de celle de la PMF.

\begin{intuitionbox}
Alors que la PMF répond à la question "Quelle est la probabilité d'obtenir \textit{exactement} $x$ ?", la CDF répond à la question "Quelle est la probabilité d'obtenir \textit{au plus} $x$ ?". C'est une fonction cumulative : pour une valeur $x$ donnée, elle additionne les probabilités de tous les résultats inférieurs ou égaux à $x$.
La CDF a toujours une forme d'escalier pour les variables discrètes. Elle commence à 0 (très loin à gauche) et monte par "sauts" à chaque valeur possible de la variable, pour finalement atteindre 1 (très loin à droite). La hauteur de chaque saut correspond à la valeur de la PMF à ce point.
\end{intuitionbox}

Traçons cette fonction "en escalier" pour notre exemple du dé.

\begin{examplebox}
Reprenons le lancer d'un dé équilibré ($X$). Calculons quelques valeurs de la CDF, notée $F(x)$.

$F(0.5) = P(X \le 0.5) = 0$

$F(1) = P(X \le 1) = P(X=1) = 1/6$

$F(1.5) = P(X \le 1.5) = P(X=1) = 1/6$

$F(2) = P(X \le 2) = P(X=1) + P(X=2) = 2/6$

$F(5.9) = P(X \le 5.9) = P(X=1) + \dots + P(X=5) = 5/6$

$F(6) = P(X \le 6) = 1$

$F(100) = P(X \le 100) = 1$
\end{examplebox}

\subsection{Variable Aléatoire Indicatrice}

Enfin, nous introduisons un outil simple mais qui s'avérera extraordinairement puissant pour les preuves, notamment celles concernant l'espérance.

\begin{definitionbox}[Variable Aléatoire Indicatrice]
La variable aléatoire indicatrice d'un événement $A$ est la variable aléatoire qui vaut 1 si $A$ se produit et 0 sinon. Nous la noterons $I_A$. Notez que $I_A \sim \text{Bern}(p)$ avec $p=P(A)$.
\end{definitionbox}

C'est un simple interrupteur "on/off".

\begin{intuitionbox}
Une variable indicatrice est un interrupteur. Elle est sur "ON" (valeur 1) si un événement qui nous intéresse se produit, et sur "OFF" (valeur 0) sinon. C'est un outil extrêmement puissant car il transforme les questions sur les probabilités des événements en questions sur les espérances des variables aléatoires, ce qui simplifie souvent les calculs.
\end{intuitionbox}

\subsection{Exercices}

% --- Concepts de Base (PMF, CDF) ---

\begin{exercicebox}[Exercice 1 : Identification de Variables Aléatoires]
Pour chacune des situations suivantes, indiquez si la variable aléatoire $X$ est discrète ou continue.
\begin{enumerate}
    \item $X$ est le nombre de Piles obtenues en lançant 10 fois une pièce.
    \item $X$ est le temps exact nécessaire pour courir un marathon.
    \item $X$ est le nombre d'emails que vous recevez un jour donné.
    \item $X$ est la température exacte à midi à Paris.
    \item $X$ est le nombre de lancers d'un dé jusqu'à obtenir un 6.
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 2 : Construction d'une PMF]
On lance un dé équilibré à 4 faces (un tétraèdre) numérotées de 1 à 4. Soit $X$ le résultat du lancer.
\begin{enumerate}
    \item Quelles sont les valeurs possibles pour $X$ ?
    \item Donnez la fonction de masse de probabilité (PMF) $P(X=k)$ pour chaque valeur $k$.
    \item Vérifiez que la somme des probabilités est égale à 1.
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 3 : PMF d'une Somme]
On lance deux dés équilibrés à 4 faces (ceux de l'exercice 2). Soit $Y$ la somme des deux résultats.
\begin{enumerate}
    \item Quelles sont les valeurs possibles pour $Y$ ?
    \item Calculez la PMF $P(Y=k)$ pour chaque valeur $k$ possible (Indice : listez les 16 issues possibles).
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 4 : Construction d'une CDF]
En utilisant la variable $Y$ et sa PMF de l'exercice 3 (somme de deux dés à 4 faces) :
\begin{enumerate}
    \item Calculez $F_Y(y) = P(Y \le y)$ pour toutes les valeurs $y$ de 2 à 8.
    \item Quelle est la valeur de $F_Y(1.5)$ ?
    \item Quelle est la valeur de $F_Y(5.2)$ ?
    \item Quelle est la valeur de $F_Y(10)$ ?
\end{enumerate}
\end{exercicebox}

% --- Loi de Bernoulli et Loi Binomiale ---

\begin{exercicebox}[Exercice 5 : Loi de Bernoulli]
Une machine produit des pièces, avec une probabilité $p=0.05$ que la pièce soit défectueuse. Soit $X$ une variable aléatoire qui vaut 1 si une pièce est défectueuse et 0 sinon.
\begin{enumerate}
    \item Quelle loi suit $X$ ? Donnez son (ou ses) paramètre(s).
    \item Quelle est la PMF de $X$ ? (Donnez $P(X=0)$ et $P(X=1)$).
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 6 : Loi Binomiale (Calcul Direct)]
On lance une pièce truquée 5 fois ($n=5$). La probabilité d'obtenir Pile (succès) est $p=0.7$. Soit $X$ le nombre de Piles obtenus.
\begin{enumerate}
    \item Quelle loi suit $X$ ? Donnez ses paramètres.
    \item Quelle est la probabilité d'obtenir exactement 3 Piles, $P(X=3)$ ?
    \item Quelle est la probabilité d'obtenir exactement 5 Piles, $P(X=5)$ ?
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 7 : Loi Binomiale (Calcul Cumulé)]
On reprend la situation de l'exercice 6 ($X \sim \text{Bin}(5, 0.7)$).
\begin{enumerate}
    \item Quelle est la probabilité d'obtenir 0 Pile, $P(X=0)$ ?
    \item En déduire la probabilité d'obtenir au moins 1 Pile, $P(X \ge 1)$.
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 8 : Problème Binomial (Contrôle Qualité)]
Un lot de 10000 articles contient 10\% d'articles défectueux. On prélève un échantillon de 20 articles \textit{avec remise} pour inspection.
Quelle est la probabilité que l'échantillon contienne exactement 2 articles défectueux ?
\end{exercicebox}

% --- Loi Hypergéométrique ---

\begin{exercicebox}[Exercice 9 : Loi Hypergéométrique (Urne)]
Une urne contient 7 boules blanches et 5 boules noires (total 12). On tire $m=4$ boules \textit{sans remise}. Soit $X$ le nombre de boules blanches tirées.
\begin{enumerate}
    \item Quelle loi suit $X$ ? Donnez ses paramètres ($w, b, m$).
    \item Quelle est la probabilité d'obtenir exactement 2 boules blanches, $P(X=2)$ ?
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 10 : Problème Hypergéométrique (Comité)]
Un département est composé de 10 hommes et 8 femmes. On choisit un comité de 6 personnes au hasard.
Quelle est la probabilité que le comité soit composé d'exactement 3 hommes et 3 femmes ?
\end{exercicebox}

\begin{exercicebox}[Exercice 11 : Binomiale vs Hypergéométrique]
Reprenons le problème de l'exercice 8 (lot de 10000 articles, 10\% défectueux), mais cette fois on prélève les 20 articles \textit{sans remise}.
\begin{enumerate}
    \item Quelle est la loi exacte du nombre $X$ d'articles défectueux ? (Donnez son nom et ses paramètres).
    \item Calculez la probabilité exacte $P(X=2)$.
    \item Comparez ce résultat à celui obtenu à l'exercice 8. L'approximation binomiale était-elle bonne ? Pourquoi ?
\end{enumerate}
\end{exercicebox}

% --- Loi Géométrique ---

\begin{exercicebox}[Exercice 12 : Loi Géométrique (Calcul Direct)]
On lance un dé équilibré à 6 faces jusqu'à obtenir un 6. Soit $X$ le nombre d'échecs \textit{avant} le premier 6.
\begin{enumerate}
    \item Quelle loi suit $X$ ? Donnez son paramètre $p$.
    \item Quelle est la probabilité que le premier 6 apparaisse au 3ème lancer ? (C'est-à-dire $P(X=2)$).
    \item Quelle est la probabilité que le premier 6 apparaisse au 1er lancer ? (C'est-à-dire $P(X=0)$).
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 13 : Loi Géométrique (Calcul Cumulé)]
Un archer touche sa cible avec une probabilité $p=0.2$ à chaque tir. Les tirs sont indépendants. Il tire jusqu'à ce qu'il touche la cible. Soit $X$ le nombre d'échecs avant son premier succès.
\begin{enumerate}
    \item Quelle est la probabilité qu'il ait besoin d'exactement 4 tirs au total ? (C'est-à-dire $P(X=3)$).
    \item Quelle est la probabilité qu'il ait besoin de plus de 2 tirs au total ? (C'est-à-dire $P(X \ge 2)$ ou $P(\text{les 2 premiers tirs sont des échecs})$).
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 14 : Variante de la Loi Géométrique]
Certains manuels définissent la loi géométrique $Y$ comme le \textit{nombre total d'essais} (et non le nombre d'échecs). Si $Y \sim \text{Geom}(p)$ selon cette définition :
\begin{enumerate}
    \item Quelle est la PMF $P(Y=k)$ pour $k=1, 2, 3, \dots$ ?
    \item En utilisant $p=1/6$ (lancer de dé), calculez $P(Y=3)$. Comparez avec $P(X=2)$ de l'exercice 12.
\end{enumerate}
\end{exercicebox}

% --- Loi de Poisson ---

\begin{exercicebox}[Exercice 15 : Loi de Poisson (Calcul Direct)]
Un centre d'appels reçoit en moyenne $\lambda = 5$ appels par heure. Soit $X$ le nombre d'appels reçus en une heure donnée. On suppose que $X$ suit une loi de Poisson.
\begin{enumerate}
    \item Quelle est la probabilité qu'il n'y ait aucun appel ($P(X=0)$) ?
    \item Quelle est la probabilité qu'il y ait exactement 5 appels ($P(X=5)$) ? (Laissez $e^{-5}$ dans votre réponse).
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 16 : Loi de Poisson (Calcul Cumulé)]
Un site web reçoit en moyenne $\lambda = 2$ visiteurs par minute. Soit $X$ le nombre de visiteurs en une minute.
Calculez la probabilité de recevoir au plus 2 visiteurs, $P(X \le 2)$. (Laissez $e^{-2}$ dans votre réponse).
\end{exercicebox}

\begin{exercicebox}[Exercice 17 : Loi de Poisson (Changement de $\lambda$)]
Un livre contient en moyenne 0.5 faute de frappe par page ($\lambda=0.5$).
\begin{enumerate}
    \item Quelle est la probabilité qu'une page donnée contienne 0 faute ?
    \item Soit $Y$ le nombre de fautes dans un chapitre de 10 pages. Quel est le nouveau paramètre $\lambda_Y$ pour $Y$ ?
    \item Quelle est la probabilité que ce chapitre de 10 pages contienne 0 faute ?
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 18 : Approximation Binomiale par Poisson]
Une compagnie d'assurance a 10000 clients ($n=10000$). La probabilité qu'un client ait un accident dans l'année est $p=0.0003$.
\begin{enumerate}
    \item Quelle est la loi exacte $X$ du nombre d'accidents ?
    \item Calculez le paramètre $\lambda = np$ pour une approximation par la loi de Poisson.
    \item En utilisant la loi de Poisson, estimez la probabilité qu'il y ait exactement 2 accidents cette année, $P(X=2)$.
\end{enumerate}
\end{exercicebox}

% --- Synthèse et Variables Indicatrices ---

\begin{exercicebox}[Exercice 19 : Choisir la Bonne Loi]
Pour chaque scénario, identifiez la loi discrète la plus appropriée (Binomiale, Hypergéométrique, Géométrique, Poisson).
\begin{enumerate}
    \item On compte le nombre de Rois en tirant 5 cartes d'un jeu, sans remise.
    \item On compte le nombre de clients arrivant à une banque entre 10h et 11h.
    \item On compte le nombre de lancers de pièce jusqu'à obtenir le premier Pile.
    \item On compte le nombre de "6" obtenus en lançant un dé 20 fois.
    \item On compte le nombre de soldats tués par ruade de cheval dans un corps d'armée en un an.
\end{enumerate}
\end{exercicebox}

\begin{exercicebox}[Exercice 20 : Variable Indicatrice]
Soit $A$ l'événement "obtenir un 6 en lançant un dé équilibré".
Soit $I_A$ la variable indicatrice de l'événement $A$.
\begin{enumerate}
    \item Quelle loi suit $I_A$ ? Donnez son nom et son paramètre.
    \item Écrivez la PMF de $I_A$.
\end{enumerate}
\end{exercicebox}

\subsection{Corrections des Exercices}

% --- Corrections : Concepts de Base (PMF, CDF) ---

\begin{correctionbox}[Correction Exercice 1 : Identification de Variables Aléatoires]
1.  \textbf{Discrète}. $X$ ne peut prendre que des valeurs entières $\{0, 1, \dots, 10\}$.
2.  \textbf{Continue}. Le temps peut prendre n'importe quelle valeur dans un intervalle (par ex. $T \in [2.5, 5]$ heures).
3.  \textbf{Discrète}. $X$ ne peut prendre que des valeurs entières $\{0, 1, 2, \dots\}$.
4.  \textbf{Continue}. La température peut prendre n'importe quelle valeur dans un intervalle (par ex. $T \in [15.0, 25.0]^\circ\text{C}$).
5.  \textbf{Discrète}. $X$ ne peut prendre que des valeurs entières $\{0, 1, 2, \dots\}$ (si on compte les échecs) ou $\{1, 2, 3, \dots\}$ (si on compte les lancers).
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 2 : Construction d'une PMF]
On lance un dé à 4 faces (1, 2, 3, 4). $X$ est le résultat.
1.  Valeurs possibles : $S_X = \{1, 2, 3, 4\}$.
2.  PMF : Le dé est équilibré, donc chaque face a la même probabilité $1/4$.
    $P(X=1) = 1/4$
    $P(X=2) = 1/4$
    $P(X=3) = 1/4$
    $P(X=4) = 1/4$
    Et $P(X=k) = 0$ pour tout autre $k$.
3.  Vérification : $\sum P(X=k) = 1/4 + 1/4 + 1/4 + 1/4 = 4/4 = 1$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 3 : PMF d'une Somme]
$Y = D_1 + D_2$, où $D_1, D_2 \in \{1, 2, 3, 4\}$. Il y a $4 \times 4 = 16$ issues équiprobables (prob. 1/16 chacune).
1.  Valeurs possibles : Min = $1+1=2$. Max = $4+4=8$. $S_Y = \{2, 3, 4, 5, 6, 7, 8\}$.
2.  PMF (en comptant les issues favorables sur 16) :
    - $P(Y=2) = P(1,1) \implies 1/16$
    - $P(Y=3) = P(1,2) + P(2,1) \implies 2/16$
    - $P(Y=4) = P(1,3) + P(2,2) + P(3,1) \implies 3/16$
    - $P(Y=5) = P(1,4) + P(2,3) + P(3,2) + P(4,1) \implies 4/16$
    - $P(Y=6) = P(2,4) + P(3,3) + P(4,2) \implies 3/16$
    - $P(Y=7) = P(3,4) + P(4,3) \implies 2/16$
    - $P(Y=8) = P(4,4) \implies 1/16$
    (Vérification : $1+2+3+4+3+2+1 = 16$. La somme est $16/16 = 1$).
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 4 : Construction d'une CDF]
On utilise la PMF de l'exercice 3. $F_Y(y) = P(Y \le y)$.
1.  CDF aux points de masse :
    - $F_Y(2) = P(Y \le 2) = P(Y=2) = 1/16$
    - $F_Y(3) = P(Y \le 3) = P(Y=2)+P(Y=3) = 1/16 + 2/16 = 3/16$
    - $F_Y(4) = P(Y \le 4) = 3/16 + P(Y=4) = 3/16 + 3/16 = 6/16$
    - $F_Y(5) = P(Y \le 5) = 6/16 + P(Y=5) = 6/16 + 4/16 = 10/16$
    - $F_Y(6) = P(Y \le 6) = 10/16 + P(Y=6) = 10/16 + 3/16 = 13/16$
    - $F_Y(7) = P(Y \le 7) = 13/16 + P(Y=7) = 13/16 + 2/16 = 15/16$
    - $F_Y(8) = P(Y \le 8) = 15/16 + P(Y=8) = 15/16 + 1/16 = 16/16 = 1$
2.  $F_Y(1.5) = P(Y \le 1.5) = 0$ (car la valeur minimale est 2).
3.  $F_Y(5.2) = P(Y \le 5.2) = P(Y \le 5) = F_Y(5) = 10/16$.
4.  $F_Y(10) = P(Y \le 10) = 1$ (car la valeur maximale est 8).
\end{correctionbox}

% --- Corrections : Loi de Bernoulli et Loi Binomiale ---

\begin{correctionbox}[Correction Exercice 5 : Loi de Bernoulli]
1.  $X$ suit une \textbf{loi de Bernoulli}. Le paramètre est $p=0.05$. On note $X \sim \text{Bern}(0.05)$.
2.  La PMF est :
    $P(X=1) = p = 0.05$ (succès = défectueux)
    $P(X=0) = 1-p = 0.95$ (échec = non défectueux)
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 6 : Loi Binomiale (Calcul Direct)]
1.  $X$ est le nombre de succès (Pile) en $n=5$ essais indépendants avec probabilité $p=0.7$.
    $X$ suit une \textbf{loi Binomiale}. $X \sim \text{Bin}(n=5, p=0.7)$.
2.  $P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$.
    $P(X=3) = \binom{5}{3} (0.7)^3 (1-0.7)^{5-3} = 10 \times (0.343) \times (0.3)^2 = 10 \times 0.343 \times 0.09 = 0.3087$.
3.  $P(X=5) = \binom{5}{5} (0.7)^5 (0.3)^0 = 1 \times (0.7)^5 \times 1 = 0.16807$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 7 : Loi Binomiale (Calcul Cumulé)]
On a $X \sim \text{Bin}(5, 0.7)$.
1.  $P(X=0) = \binom{5}{0} (0.7)^0 (0.3)^5 = 1 \times 1 \times (0.3)^5 = 0.00243$.
2.  L'événement "au moins 1 Pile" ($X \ge 1$) est le complémentaire de "0 Pile" ($X=0$).
    $P(X \ge 1) = 1 - P(X=0) = 1 - 0.00243 = 0.99757$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 8 : Problème Binomial (Contrôle Qualité)]
Le tirage est \textit{avec remise}, donc les essais sont indépendants. C'est une loi binomiale.
$n = 20$ (nombre d'essais).
$p = 0.10$ (probabilité de succès = défectueux).
On cherche $P(X=2)$.
$P(X=2) = \binom{20}{2} (0.1)^2 (1-0.1)^{20-2}$
$P(X=2) = \frac{20 \times 19}{2} (0.1)^2 (0.9)^{18} = 190 \times 0.01 \times (0.9)^{18}$
$P(X=2) = 1.9 \times (0.9)^{18} \approx 1.9 \times 0.15009 \approx 0.2852$.
\end{correctionbox}

% --- Corrections : Loi Hypergéométrique ---

\begin{correctionbox}[Correction Exercice 9 : Loi Hypergéométrique (Urne)]
Le tirage est \textit{sans remise} d'une population finie.
1.  $X$ suit une \textbf{loi Hypergéométrique}.
    Paramètres : $w=7$ (blanches, succès), $b=5$ (noires, échecs), $m=4$ (nombre de tirages).
    $X \sim \text{HG}(w=7, b=5, m=4)$.
2.  On cherche $P(X=2)$.
    $P(X=k) = \frac{\binom{w}{k} \binom{b}{m-k}}{\binom{w+b}{m}}$
    $P(X=2) = \frac{\binom{7}{2} \binom{5}{4-2}}{\binom{12}{4}} = \frac{\binom{7}{2} \binom{5}{2}}{\binom{12}{4}}$
    $P(X=2) = \frac{(\frac{7 \times 6}{2}) \times (\frac{5 \times 4}{2})}{(\frac{12 \times 11 \times 10 \times 9}{4 \times 3 \times 2 \times 1})} = \frac{21 \times 10}{495} = \frac{210}{495} = \frac{14}{33} \approx 0.4242$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 10 : Problème Hypergéométrique (Comité)]
Tirage sans remise. C'est une loi Hypergéométrique.
$w=10$ (hommes), $b=8$ (femmes), $m=6$ (taille du comité). Total $N=18$.
On cherche $P(X=3)$ (exactement 3 hommes, ce qui implique $m-k = 6-3=3$ femmes).
$P(X=3) = \frac{\binom{10}{3} \binom{8}{3}}{\binom{18}{6}}$
$P(X=3) = \frac{(\frac{10 \times 9 \times 8}{3 \times 2 \times 1}) \times (\frac{8 \times 7 \times 6}{3 \times 2 \times 1})}{(\frac{18 \times 17 \times 16 \times 15 \times 14 \times 13}{6 \times 5 \times 4 \times 3 \times 2 \times 1})} = \frac{120 \times 56}{18564} = \frac{6720}{18564} \approx 0.362$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 11 : Binomiale vs Hypergéométrique]
Population totale $N=10000$. 10\% défectueux, donc $w=1000$ (défectueux), $b=9000$ (non défectueux).
Tirage de $m=20$ \textit{sans remise}.
1.  Loi exacte : \textbf{Loi Hypergéométrique}.
    $X \sim \text{HG}(w=1000, b=9000, m=20)$.
2.  Probabilité exacte $P(X=2)$ :
    $P(X=2) = \frac{\binom{1000}{2} \binom{9000}{18}}{\binom{10000}{20}}$
    $P(X=2) = \frac{(\frac{1000 \times 999}{2}) \times (\frac{9000 \times \dots \times 8983}{18!})}{(\frac{10000 \times \dots \times 9981}{20!})} \approx 0.2854$.
    (Le calcul est très complexe, mais on peut montrer qu'il est très proche de la binomiale).
3.  Le résultat de l'exercice 8 (Binomiale) était $\approx 0.2852$.
    L'approximation binomiale est excellente. La raison est que la taille de l'échantillon ($m=20$) est très petite par rapport à la taille de la population ($N=10000$). Le fait de ne pas remettre les 20 articles change à peine les probabilités pour les tirages suivants.
\end{correctionbox}

% --- Corrections : Loi Géométrique ---

\begin{correctionbox}[Correction Exercice 12 : Loi Géométrique (Calcul Direct)]
1.  $X$ est le nombre d'échecs avant le premier succès. $X$ suit une \textbf{loi Géométrique}.
    Le succès est "obtenir 6", donc $p = 1/6$. $X \sim \text{Geom}(p=1/6)$.
2.  "Premier 6 au 3ème lancer" signifie 2 échecs (lancers 1 et 2) puis 1 succès (lancer 3).
    C'est $P(X=2)$. $q = 1-p = 5/6$.
    $P(X=2) = q^2 p^1 = (5/6)^2 (1/6) = 25/216 \approx 0.1157$.
3.  "Premier 6 au 1er lancer" signifie 0 échec. C'est $P(X=0)$.
    $P(X=0) = q^0 p^1 = 1 \times (1/6) = 1/6$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 13 : Loi Géométrique (Calcul Cumulé)]
$p=0.2$ (succès), $q=0.8$ (échec). $X$ compte les échecs. $X \sim \text{Geom}(0.2)$.
1.  "Exactement 4 tirs au total" signifie 3 échecs suivis d'un succès. On cherche $P(X=3)$.
    $P(X=3) = q^3 p^1 = (0.8)^3 (0.2) = 0.512 \times 0.2 = 0.1024$.
2.  "Plus de 2 tirs au total" signifie qu'il faut au moins 3 tirs. C'est l'événement "les 2 premiers tirs sont des échecs".
    La probabilité est $P(\text{Echec 1} \cap \text{Echec 2}) = q \times q = q^2$.
    $P(X \ge 2) = (0.8)^2 = 0.64$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 14 : Variante de la Loi Géométrique]
$Y$ est le nombre total d'essais ($k=1, 2, 3, \dots$). $p$ est la prob. de succès.
1.  Pour que $Y=k$, il faut $k-1$ échecs, suivis d'un succès.
    $P(Y=k) = (1-p)^{k-1} p = q^{k-1} p$, pour $k=1, 2, \dots$
2.  Avec $p=1/6$, on cherche $P(Y=3)$.
    $P(Y=3) = (5/6)^{3-1} (1/6) = (5/6)^2 (1/6) = 25/216$.
    C'est le même résultat que $P(X=2)$ de l'exercice 12. Les deux définitions décrivent la même situation (3 lancers au total).
\end{correctionbox}

% --- Corrections : Loi de Poisson ---

\begin{correctionbox}[Correction Exercice 15 : Loi de Poisson (Calcul Direct)]
$X \sim \text{Poisson}(\lambda=5)$. PMF : $P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!}$.
1.  $P(X=0) = \frac{e^{-5} 5^0}{0!} = \frac{e^{-5} \times 1}{1} = e^{-5} \approx 0.0067$.
2.  $P(X=5) = \frac{e^{-5} 5^5}{5!} = \frac{e^{-5} \times 3125}{120} = e^{-5} \times \frac{625}{24} \approx 26.04 \times e^{-5} \approx 0.1755$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 16 : Loi de Poisson (Calcul Cumulé)]
$X \sim \text{Poisson}(\lambda=2)$. On cherche $P(X \le 2)$.
$P(X \le 2) = P(X=0) + P(X=1) + P(X=2)$
$P(X=0) = \frac{e^{-2} 2^0}{0!} = e^{-2}$
$P(X=1) = \frac{e^{-2} 2^1}{1!} = 2e^{-2}$
$P(X=2) = \frac{e^{-2} 2^2}{2!} = \frac{4e^{-2}}{2} = 2e^{-2}$
$P(X \le 2) = e^{-2} + 2e^{-2} + 2e^{-2} = 5e^{-2} \approx 5 \times 0.1353 = 0.6767$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 17 : Loi de Poisson (Changement de $\lambda$)]
1.  Pour une page, $X \sim \text{Poisson}(\lambda=0.5)$.
    $P(X=0) = \frac{e^{-0.5} (0.5)^0}{0!} = e^{-0.5} \approx 0.6065$.
2.  Si le taux est 0.5 faute/page, le taux pour 10 pages est $\lambda_Y = 0.5 \times 10 = 5$.
    $Y \sim \text{Poisson}(\lambda_Y=5)$.
3.  On cherche $P(Y=0)$.
    $P(Y=0) = \frac{e^{-5} 5^0}{0!} = e^{-5} \approx 0.0067$.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 18 : Approximation Binomiale par Poisson]
1.  C'est un tirage de $n=10000$ clients, où chaque client est un essai de Bernoulli avec $p=0.0003$. La loi exacte est $X \sim \text{Bin}(10000, 0.0003)$.
2.  Le paramètre $\lambda$ pour l'approximation Poisson est $\lambda = np = 10000 \times 0.0003 = 3$.
3.  On utilise $Y \sim \text{Poisson}(\lambda=3)$ pour approximer $X$.
    $P(X=2) \approx P(Y=2) = \frac{e^{-3} 3^2}{2!} = \frac{9e^{-3}}{2} = 4.5 e^{-3} \approx 4.5 \times 0.04979 \approx 0.224$.
\end{correctionbox}

% --- Corrections : Synthèse et Variables Indicatrices ---

\begin{correctionbox}[Correction Exercice 19 : Choisir la Bonne Loi]
1.  Tirage sans remise d'une population finie : \textbf{Loi Hypergéométrique}.
2.  Comptage d'événements sur un intervalle de temps fixe : \textbf{Loi de Poisson}.
3.  Comptage d'essais jusqu'au premier succès : \textbf{Loi Géométrique}.
4.  Comptage de succès sur un nombre fixe d'essais indépendants : \textbf{Loi Binomiale}.
5.  Comptage d'événements rares sur un intervalle (temps/espace) : \textbf{Loi de Poisson}.
\end{correctionbox}

\begin{correctionbox}[Correction Exercice 20 : Variable Indicatrice]
$A$ = "obtenir 6". $P(A) = 1/6$.
$I_A = 1$ si $A$ se produit, $I_A = 0$ sinon.
1.  C'est une expérience avec deux issues (succès/échec). $I_A$ suit une \textbf{Loi de Bernoulli}.
    Le paramètre est $p = P(A) = 1/6$. $I_A \sim \text{Bern}(1/6)$.
2.  La PMF de $I_A$ est :
    $P(I_A = 1) = p = 1/6$
    $P(I_A = 0) = 1-p = 5/6$
\end{correctionbox}