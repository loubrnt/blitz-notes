\newpage
\section{Variables Aléatoires Discrètes}

\subsection{Variable Aléatoire}

Jusqu'à présent, nous avons parlé d'événements (comme "obtenir Pile" ou "tirer un Roi"). Pour analyser ces phénomènes avec des outils mathématiques plus puissants, nous devons traduire ces résultats concrets en nombres. C'est le rôle de la variable aléatoire.

\begin{definitionbox}[Variable Aléatoire]
Étant donné une expérience avec un univers $S$, une variable aléatoire est une fonction de l'univers $S$ vers les nombres réels $\mathbb{R}$.
\end{definitionbox}

Cette définition formelle masque une idée très simple :

\begin{intuitionbox}
Une variable aléatoire est une manière de traduire les résultats d'une expérience en nombres. Au lieu de travailler avec des concepts comme "Pile" ou "Face", on leur assigne des valeurs numériques (par exemple, 1 pour Pile, 0 pour Face). Cela nous permet d'utiliser toute la puissance des outils mathématiques (fonctions, calculs, etc.) pour analyser le hasard. C'est un pont entre le monde concret des événements et le monde abstrait des nombres.
\end{intuitionbox}

Prenons un exemple classique :

\begin{examplebox}
On lance deux dés. L'univers $S$ est l'ensemble des 36 paires de résultats, comme $(1,1), (1,2), \dots, (6,6)$. On peut définir une variable aléatoire $X$ comme étant la \textbf{somme des deux dés}.
Pour le résultat $(2, 5)$, la valeur de la variable aléatoire est $X(2, 5) = 2 + 5 = 7$.
\end{examplebox}

\subsection{Variable Aléatoire Discrète}

Les variables aléatoires peuvent être de différents types. Nous commençons par le type le plus simple à "compter".

\begin{definitionbox}[Variable Aléatoire Discrète]
Une variable aléatoire $X$ est dite discrète s'il existe une liste finie ou infinie dénombrable de valeurs $a_1, a_2, \dots$ telle que $P(X=a_j \text{ pour un certain } j) = 1$.
\end{definitionbox}

L'analogie la plus simple pour comprendre le terme "discret" est celle d'un escalier.

\begin{intuitionbox}
Une variable aléatoire est "discrète" si on peut lister (compter) toutes les valeurs qu'elle peut prendre, même si cette liste est infinie. Pensez aux "sauts" d'une valeur à l'autre, sans possibilité de prendre une valeur intermédiaire. C'est comme monter un escalier : on peut être sur la marche 1, 2 ou 3, mais jamais sur la marche 2.5. Le nombre de têtes en 10 lancers, le résultat d'un dé, le nombre d'emails que vous recevez en une heure sont des exemples. À l'opposé, une variable continue pourrait être la taille exacte d'une personne, qui peut prendre n'importe quelle valeur dans un intervalle.
\end{intuitionbox}

\subsection{Fonction de Masse (PMF)}

Maintenant que nous avons une variable aléatoire qui produit des nombres discrets, nous avons besoin d'une fonction pour décrire la probabilité de chacun de ces nombres.

\begin{definitionbox}[Probability Mass Function (PMF)]
La fonction de masse (PMF) d'une variable aléatoire discrète $X$ est la fonction $P_X$ donnée par $P_X(x) = P(X=x)$.
\end{definitionbox}

C'est la "carte d'identité" probabiliste de la variable :

\begin{intuitionbox}
La PMF est la "carte d'identité" probabiliste d'une variable aléatoire discrète. Pour chaque valeur que la variable peut prendre, la PMF nous donne la probabilité exacte associée à cette valeur. C'est comme si chaque résultat possible avait une "étiquette de prix" qui indique sa chance de se produire. La somme de toutes ces probabilités doit bien sûr valoir 1.
\end{intuitionbox}

Un exemple très simple est le lancer de dé :

\begin{examplebox}
Soit $X$ le résultat d'un lancer de dé équilibré. La variable $X$ peut prendre les valeurs $\{1, 2, 3, 4, 5, 6\}$.
La PMF de $X$ est la fonction qui assigne $1/6$ à chaque valeur :
$P(X=1) = 1/6$, $P(X=2) = 1/6$, ..., $P(X=6) = 1/6$.
Pour toute autre valeur $x$ (par exemple $x=2.5$ ou $x=7$), $P(X=x) = 0$.
\end{examplebox}

\subsection{Loi de Bernoulli}

Commençons par la loi de probabilité discrète la plus simple.

\begin{definitionbox}[Distribution de Bernoulli]
Une variable aléatoire $X$ suit la distribution de Bernoulli avec paramètre $p$ si $P(X=1) = p$ et $P(X=0) = 1-p$, où $0 < p < 1$. On note cela $X \sim \text{Bern}(p)$.
\end{definitionbox}

C'est la brique fondamentale de nombreuses autres distributions.

\begin{intuitionbox}
La distribution de Bernoulli est le modèle le plus simple pour une expérience aléatoire avec seulement deux issues : "succès" (codé par 1) et "échec" (codé par 0). C'est la brique de base de nombreuses autres distributions. Pensez à un unique lancer de pièce (Pile/Face), un unique tir au but (Marqué/Manqué), ou la réponse à une question par oui/non. Le paramètre $p$ est simplement la probabilité du "succès".
\end{intuitionbox}

\subsection{Loi Binomiale}

Que se passe-t-il si nous répétons une expérience de Bernoulli $n$ fois et que nous comptons le nombre total de succès ?

\begin{theorembox}[PMF Binomiale]
Si $X \sim \text{Bin}(n, p)$, alors la PMF de $X$ est :
$$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
pour $k = 0, 1, \dots, n$.
\end{theorembox}

La preuve de cette formule est un argument combinatoire direct.

\begin{proofbox}
Nous voulons trouver la probabilité d'obtenir exactement $k$ succès au cours de $n$ essais indépendants.
\begin{enumerate}
    \item \textbf{Probabilité d'une séquence spécifique :} Considérons d'abord une séquence spécifique contenant $k$ succès (S) et $n-k$ échecs (E), par exemple $S, S, \dots, S, E, E, \dots, E$.
    Puisque les essais sont indépendants, la probabilité de cette séquence est le produit des probabilités individuelles :
    $$ \underbrace{p \times p \times \dots \times p}_{k \text{ fois}} \times \underbrace{(1-p) \times \dots \times (1-p)}_{n-k \text{ fois}} = p^k (1-p)^{n-k} $$
    
    \item \textbf{Nombre de séquences possibles :} La séquence ci-dessus n'est qu'une des nombreuses façons d'obtenir $k$ succès. Le nombre total de façons d'arranger $k$ succès parmi $n$ positions (essais) est donné par le coefficient binomial $\binom{n}{k}$.
    
    \item \textbf{Probabilité totale :} Chacune de ces $\binom{n}{k}$ séquences a la même probabilité $p^k (1-p)^{n-k}$. Puisque toutes ces séquences sont des événements disjoints, la probabilité totale d'obtenir $k$ succès (dans n'importe quel ordre) est la somme de leurs probabilités :
    $$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
\end{enumerate}
\end{proofbox}

Chaque partie de cette formule a une signification logique claire.

\begin{intuitionbox}
La distribution binomiale répond à la question : "Si je répète $n$ fois la même expérience de Bernoulli (qui a une probabilité de succès $p$), quelle est la probabilité d'obtenir exactement $k$ succès ?"
La formule est construite logiquement en multipliant trois composantes. D'abord, $\mathbf{p^k}$ représente la probabilité d'obtenir $k$ succès. Ensuite, $\mathbf{(1-p)^{n-k}}$ est la probabilité que les $n-k$ échecs restants se produisent. Finalement, comme les $k$ succès peuvent apparaître n'importe où parmi les $n$ essais, on multiplie par $\mathbf{\binom{n}{k}}$, qui compte le nombre de manières distinctes de placer ces succès.
\end{intuitionbox}

Appliquons cela à un exemple classique :

\begin{examplebox}
On lance une pièce équilibrée 10 fois ($n=10$, $p=0.5$). Quelle est la probabilité d'obtenir exactement 6 Piles ($k=6$) ?
$$ P(X=6) = \binom{10}{6} (0.5)^6 (1-0.5)^{10-6} = \frac{10!}{6!4!} (0.5)^{10} = 210 \times (0.5)^{10} \approx 0.205 $$
Il y a environ 20.5\% de chance d'obtenir exactement 6 Piles.
\end{examplebox}

\subsection{Loi Hypergéométrique}

La loi binomiale suppose que les essais sont indépendants, ce qui est vrai si l'on tire *avec remise*. Que se passe-t-il si l'on tire *sans remise* ?

\begin{theorembox}[PMF Hypergéométrique]
Si $X \sim \text{HG}(w, b, m)$, alors la PMF de $X$ est :
$$ P(X=k) = \frac{\binom{w}{k} \binom{b}{m-k}}{\binom{w+b}{m}} $$
\end{theorembox}

La preuve de cette formule est un argument de dénombrement pur, basé sur la définition naïve de la probabilité.

\begin{proofbox}
Nous utilisons la définition naïve $P(A) = |A| / |S|$.
Nous tirons $m$ boules d'une urne contenant $w$ blanches et $b$ noires, soit $w+b$ boules au total.

\begin{enumerate}
    \item \textbf{Taille de l'univers ($|S|$)} : Le nombre total de façons de choisir $m$ boules parmi $w+b$ est $\binom{w+b}{m}$.
    
    \item \textbf{Taille de l'événement favorable ($|A|$)} : Nous voulons l'événement $A$ = "obtenir exactement $k$ boules blanches ET $m-k$ boules noires".
    \begin{itemize}
        \item Le nombre de façons de choisir $k$ blanches parmi $w$ est $\binom{w}{k}$.
        \item Le nombre de façons de choisir $m-k$ noires parmi $b$ est $\binom{b}{m-k}$.
    \end{itemize}
    Par le principe de la multiplication (dénombrement), le nombre total de façons de réaliser $A$ est $|A| = \binom{w}{k} \binom{b}{m-k}$.
    
    \item \textbf{Probabilité :} En divisant le nombre d'issues favorables par le nombre total d'issues, on obtient :
    $$ P(X=k) = \frac{|A|}{|S|} = \frac{\binom{w}{k} \binom{b}{m-k}}{\binom{w+b}{m}} $$
\end{enumerate}
\end{proofbox}

Chaque terme de cette fraction a un sens très concret :

\begin{intuitionbox}
La distribution hypergéométrique est la "cousine" de la binomiale pour les tirages \textbf{sans remise}. Imaginez une urne avec des boules de deux couleurs (par exemple, $w$ blanches et $b$ noires). Vous tirez $m$ boules d'un coup. Quelle est la probabilité que vous ayez exactement $k$ boules blanches ?
La formule est un simple ratio issu du dénombrement. Le \textbf{dénominateur}, $\binom{w+b}{m}$, compte le nombre total de façons de tirer $m$ boules parmi toutes celles disponibles. Le \textbf{numérateur} compte les issues favorables : c'est le produit du nombre de façons de choisir $k$ blanches parmi les $w$ ($\binom{w}{k}$) ET de choisir les $m-k$ boules restantes parmi les noires ($\binom{b}{m-k}$). La différence clé avec la loi binomiale est que les tirages ne sont pas indépendants.
\end{intuitionbox}

Un exemple typique est la formation de comités à partir d'un groupe.

\begin{examplebox}
Un comité de 5 personnes est choisi au hasard parmi un groupe de 8 hommes et 10 femmes. Quelle est la probabilité que le comité soit composé de 2 hommes et 3 femmes ?
Ici, on tire 5 personnes ($m=5$) d'une population de 18 personnes. On s'intéresse au nombre d'hommes ($k=2$) parmi les 8 disponibles ($w=8$). Le reste du comité sera composé de femmes ($b=10$).
$$ P(X=2) = \frac{\binom{8}{2} \binom{10}{3}}{\binom{18}{5}} = \frac{28 \times 120}{8568} \approx 0.392 $$
Il y a environ 39.2\% de chance que le comité ait exactement cette composition.
\end{examplebox}

\subsection{Loi Géométrique}

Revenons aux essais de Bernoulli (indépendants). Au lieu de fixer le nombre d'essais $n$, demandons-nous : combien d'essais faut-il avant d'obtenir notre premier succès ?

\begin{theorembox}[PMF de la loi géométrique]
Une variable aléatoire $X$ suit la loi géométrique de paramètre $p$, notée $X \sim \text{Geom}(p)$, si elle modélise le nombre d'échecs avant le premier succès dans une série d'épreuves de Bernoulli indépendantes. Sa fonction de masse (PMF) est :
$$ P(X=k) = (1-p)^k p \quad \text{pour } k=0, 1, 2, \dots $$
où $q = 1-p$ est la probabilité d'échec.
\end{theorembox}

La preuve de cette formule est une application directe de l'indépendance des essais.

\begin{proofbox}
Soit $S_i$ l'événement "succès au $i$-ème essai" et $E_i$ l'événement "échec au $i$-ème essai".
L'événement $\{X=k\}$ signifie que nous avons observé exactement $k$ échecs, suivis d'un succès au $(k+1)$-ème essai.
C'est la séquence d'événements : $E_1 \cap E_2 \cap \dots \cap E_k \cap S_{k+1}$.

Puisque tous les essais sont indépendants, la probabilité de cette intersection est le produit des probabilités individuelles :
\begin{align*}
P(X=k) &= P(E_1) \times P(E_2) \times \dots \times P(E_k) \times P(S_{k+1}) \\
&= \underbrace{(1-p) \times (1-p) \times \dots \times (1-p)}_{k \text{ fois}} \times p \\
&= (1-p)^k p
\end{align*}
\end{proofbox}

La formule est donc très littérale :

\begin{intuitionbox}
La formule $P(X=k) = q^k p$ décrit la probabilité d'une séquence très spécifique : $k$ échecs consécutifs (chacun avec une probabilité $q$, donc $q^k$ pour la série), suivis immédiatement d'un succès (avec une probabilité $p$). C'est la loi de "l'attente du premier succès".
\end{intuitionbox}

Un exemple classique est l'attente d'un résultat spécifique sur un dé.

\begin{examplebox}[Premier 6 au lancer de dé]
On lance un dé jusqu'à obtenir un 6. La probabilité de succès est $p=1/6$, et celle d'échec est $q=5/6$. Quelle est la probabilité que l'on ait besoin de 3 lancers (donc 2 échecs avant le premier succès) ?
Ici, $k=2$. La probabilité est :
$$ P(X=2) = (5/6)^2 \cdot (1/6) = \frac{25}{216} \approx 0.116 $$
\end{examplebox}

\subsection{Loi de Poisson}

Introduisons maintenant une loi utilisée pour modéliser le nombre d'événements se produisant dans un intervalle de temps ou d'espace fixe.

\begin{definitionbox}[Distribution de Poisson]
Une variable aléatoire $X$ suit la loi de Poisson de paramètre $\lambda > 0$ si sa PMF est donnée par :
$$ P(X=k) = \frac{e^{-\lambda} \lambda^k}{k!} \quad \text{pour } k=0, 1, 2, \dots $$
Elle modélise typiquement le nombre d'événements se produisant dans un intervalle de temps ou d'espace fixe.
\end{definitionbox}

Cette loi est souvent appelée la loi des événements rares.

\begin{intuitionbox}
La loi de Poisson est la loi des événements rares. Imaginez que vous comptez le nombre d'appels arrivant à un standard téléphonique en une minute. Il y a de nombreux instants où un appel pourrait arriver, mais la probabilité à chaque instant est infime. La loi de Poisson modélise ce type de scénario, où l'on connaît seulement le taux moyen d'arrivée des événements ($\lambda$).
\end{intuitionbox}

Mais d'où vient cette formule avec $e$ et une factorielle ? Elle vient d'une approximation de la loi binomiale lorsque $n$ est très grand et $p$ très petit.

\begin{theorembox}[La loi de Poisson comme limite de la loi binomiale]
Soit $X_n \sim \text{Bin}(n, p_n)$, où $\lambda = np_n$ est une constante positive fixée. Alors, pour tout $k \in \{0, 1, 2, \dots\}$, nous avons :
$$ \lim_{n \to \infty} P(X_n=k) = \frac{e^{-\lambda}\lambda^k}{k!} $$
En pratique, la loi de Poisson est une excellente approximation de la loi binomiale quand $n$ est grand et $p$ est petit.
\end{theorembox}

% --- INTUITION AMÉLIORÉE ---
\begin{intuitionbox}[Convergence Binomiale vers Poisson : L'Exemple des Naissances]
Supposons que les bébés naissent dans une grande ville à un taux moyen de $\lambda=10$ naissances par jour. Comment modéliser le nombre $X$ de naissances un jour donné ?

\textbf{1. Approche Binomiale (Découpage du Temps) :}
On peut diviser la journée (24h) en $n$ très petits intervalles de temps (par exemple, $n = 24 \times 60 \times 60 = 86400$ secondes).
\begin{itemize}
    \item Si $n$ est très grand, la chance $p$ qu'une naissance se produise \textit{exactement} pendant une seconde donnée est minuscule. On peut calculer cette probabilité $p$ comme le taux moyen divisé par le nombre d'intervalles : $p = \lambda / n = 10 / 86400$.
    \item On peut aussi supposer que la probabilité d'avoir *deux* naissances ou plus dans la même seconde est négligeable. Chaque seconde est donc comme un mini-essai de Bernoulli : soit 1 naissance (avec probabilité $p$), soit 0 naissance (avec probabilité $1-p$).
    \item Le nombre total de naissances $X$ sur la journée est la somme de ces $n$ essais de Bernoulli quasi-indépendants. $X$ suit donc approximativement une loi binomiale : $X \approx \text{Bin}(n, p=\lambda/n)$.
\end{itemize}
La probabilité d'avoir exactement $k$ naissances serait $P(X=k) \approx \binom{n}{k} p^k (1-p)^{n-k}$.

\textbf{2. Le Passage à la Limite (Modèle Continu) :}
Que se passe-t-il si on rend les intervalles de temps infiniment petits ($n \to \infty$) ? C'est là que la magie opère :
\begin{itemize}
    \item Le terme $\binom{n}{k}$ (combien de façons de choisir $k$ secondes parmi $n$) se comporte comme $n^k/k!$ pour $n$ grand.
    \item Le terme $p^k = (\lambda/n)^k$ devient $\lambda^k / n^k$.
    \item Le terme $(1-p)^{n-k} = (1-\lambda/n)^{n-k}$. Comme $k$ est petit par rapport à $n$, ceci est très proche de $(1-\lambda/n)^n$, qui tend vers $e^{-\lambda}$.
\end{itemize}
En combinant ces approximations (expliquées plus en détail dans la preuve formelle), on trouve que la probabilité $P(X=k)$ tend vers $\frac{n^k}{k!} \frac{\lambda^k}{n^k} e^{-\lambda} = \frac{e^{-\lambda}\lambda^k}{k!}$.

\textbf{Conclusion :}
La loi de Poisson apparaît naturellement comme la limite d'un processus binomial où l'on a un très grand nombre d'opportunités ($n$) pour qu'un événement rare (probabilité $p$) se produise, tout en maintenant un taux moyen constant ($\lambda = np$).
\end{intuitionbox}
% --- FIN INTUITION AMÉLIORÉE ---

La preuve formelle montre comment les termes de la formule binomiale se transforment en ceux de la formule de Poisson lorsque $n \to \infty$.

% --- PROOFBOX (IDENTIQUE À LA PRÉCÉDENTE) ---
\begin{proofbox}[Dérivation de la loi de Poisson à partir de la loi Binomiale (Détaillée)]
On part de la fonction de masse (PMF) d'une variable aléatoire $X_n$ suivant une loi binomiale $\text{Bin}(n, p)$, où l'on pose $p = \lambda/n$. L'objectif est de trouver la limite de cette PMF lorsque $n$ tend vers l'infini, tout en gardant $\lambda = np$ constant (ce qui implique que $p$ doit tendre vers 0).

La PMF binomiale est :
$$ P(X_n=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
Substituons $p = \lambda/n$ :
$$ P(X_n=k) = \binom{n}{k} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} $$
Maintenant, développons le coefficient binomial $\binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n(n-1)(n-2)\cdots(n-k+1)}{k!}$ :
$$ P(X_n=k) = \frac{n(n-1)\cdots(n-k+1)}{k!} \left(\frac{\lambda}{n}\right)^k \left(1-\frac{\lambda}{n}\right)^{n-k} $$
Réorganisons les termes pour isoler ceux qui dépendent de $n$ :
$$ P(X_n=k) = \frac{\lambda^k}{k!} \times \frac{n(n-1)\cdots(n-k+1)}{n^k} \times \left(1-\frac{\lambda}{n}\right)^n \times \left(1-\frac{\lambda}{n}\right)^{-k} $$
Nous allons maintenant examiner la limite de chaque partie lorsque $n \to \infty$, pour $k$ et $\lambda$ fixés.

\begin{enumerate}
    \item $\mathbf{\frac{\lambda^k}{k!}}$ : Ce terme est constant par rapport à $n$, donc sa limite est lui-même.
    
    \item $\mathbf{\frac{n(n-1)\cdots(n-k+1)}{n^k}}$ : Ce terme est un produit de $k$ facteurs divisé par $n^k$. On peut le réécrire comme :
    $$ \frac{n}{n} \times \frac{n-1}{n} \times \frac{n-2}{n} \times \cdots \times \frac{n-k+1}{n} $$
    $$ = 1 \times \left(1-\frac{1}{n}\right) \times \left(1-\frac{2}{n}\right) \times \cdots \times \left(1-\frac{k-1}{n}\right) $$
    Lorsque $n \to \infty$, chacun des termes $\frac{1}{n}, \frac{2}{n}, \dots, \frac{k-1}{n}$ tend vers 0 (car $k$ est fixe). Donc, chaque parenthèse tend vers $(1-0)=1$. Puisqu'il y a un nombre \textit{fixe} $k$ de termes dans le produit, la limite du produit est le produit des limites :
    $$ \lim_{n \to \infty} \frac{n(n-1)\cdots(n-k+1)}{n^k} = 1 \times 1 \times \cdots \times 1 = 1 $$
    \textit{Intuition : Pour $n$ très grand par rapport à $k$, les $k$ termes $n, n-1, \dots, n-k+1$ sont tous "presque" égaux à $n$. Leur produit est donc "presque" $n^k$, et le ratio est "presque" 1.}

    \item $\mathbf{\left(1-\frac{\lambda}{n}\right)^n}$ : C'est une limite fondamentale en analyse. On sait que pour tout réel $x$, $\lim_{n \to \infty} (1 + x/n)^n = e^x$. Ici, nous avons $x = -\lambda$. Donc :
    $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^n = e^{-\lambda} $$
    \textit{Intuition : C'est la définition même de l'exponentielle comme limite d'intérêts composés continus (ici, avec un taux négatif).}

    \item $\mathbf{\left(1-\frac{\lambda}{n}\right)^{-k}}$ : Lorsque $n \to \infty$, le terme $\lambda/n$ tend vers 0. L'expression à l'intérieur de la parenthèse tend donc vers $(1-0) = 1$. Puisque $k$ est un exposant fixe :
    $$ \lim_{n \to \infty} \left(1-\frac{\lambda}{n}\right)^{-k} = 1^{-k} = 1 $$
    \textit{Intuition : Pour $n$ très grand, $(1-\lambda/n)$ est très proche de 1. Élever ce nombre très proche de 1 à une puissance fixe $k$ le laisse très proche de 1.}
\end{enumerate}
Finalement, en multipliant les limites de chaque partie (puisque la limite d'un produit est le produit des limites) :
$$ \lim_{n \to \infty} P(X_n=k) = \left(\frac{\lambda^k}{k!}\right) \times (1) \times (e^{-\lambda}) \times (1) = \frac{e^{-\lambda}\lambda^k}{k!} $$
Ceci est exactement la fonction de masse de probabilité d'une loi de Poisson de paramètre $\lambda$.
\end{proofbox}
% --- FIN PROOFBOX ---


Un ensemble de données historiques célèbres illustre parfaitement cette loi.

\begin{examplebox}[Décès par ruade de cheval : Les données de Bortkiewicz]
En 1898, le statisticien Ladislaus Bortkiewicz a publié des données célèbres sur le nombre de soldats de la cavalerie prussienne tués par des ruades de cheval. Ces données sont un exemple classique d'application de la loi de Poisson pour modéliser des événements rares.

\noindent\textbf{Contexte et calcul du paramètre $\lambda$ :}
Sur une période de 20 ans, en observant 10 corps d'armée, il a collecté des données sur 200 "corps-années". Durant cette période, il y a eu un total de 122 décès. Le taux moyen de décès par corps-année est donc :
$$ \lambda = \frac{\text{Nombre total de décès}}{\text{Nombre total de corps-années}} = \frac{122}{200} = 0.61 $$
Le nombre de décès par corps-année, $X$, est donc modélisé par une loi de Poisson : $X \sim \text{Poisson}(\lambda=0.61)$.

\noindent\textbf{Comparaison des données observées et des prédictions du modèle :}
On peut calculer la probabilité d'observer $k$ décès en une année-corps en utilisant la PMF de Poisson : $P(X=k) = \frac{e^{-0.61} (0.61)^k}{k!}$. En multipliant cette probabilité par le nombre total d'observations (200), on obtient le nombre de cas attendus (nombre de corps d'armes dans lequels il y a k deces).

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Nombre de décès (k)} & \textbf{Observé} & \textbf{Probabilité de Poisson} & \textbf{Attendu} \\
\hline
0 & 109 & $P(X=0) \approx 0.543$ & 108.7 \\
1 & 65 & $P(X=1) \approx 0.331$ & 66.3 \\
2 & 22 & $P(X=2) \approx 0.101$ & 20.2 \\
3 & 3 & $P(X=3) \approx 0.021$ & 4.1 \\
4 & 1 & $P(X=4) \approx 0.003$ & 0.6 \\
5+ & 0 & $P(X \ge 5) \approx 0.000$ & 0.0 \\
\hline
\end{tabular}
\end{center}

L'adéquation remarquable entre les fréquences observées et les valeurs attendues par le modèle de Poisson a contribué à populariser cette distribution pour l'analyse d'événements rares.
\end{examplebox}

\subsection{Fonction de Répartition (CDF)}

Nous avons la PMF, qui donne $P(X=x)$. Une autre fonction tout aussi importante est la fonction de répartition (CDF), qui "accumule" ces probabilités.

\begin{definitionbox}[Cumulative Distribution Function (CDF)]
La fonction de répartition (CDF) d'une variable aléatoire $X$ est la fonction $F_X$ donnée par $F_X(x) = P(X \le x)$.
\end{definitionbox}

Cette fonction répond à une question différente de celle de la PMF.

\begin{intuitionbox}
Alors que la PMF répond à la question "Quelle est la probabilité d'obtenir \textit{exactement} $x$ ?", la CDF répond à la question "Quelle est la probabilité d'obtenir \textit{au plus} $x$ ?". C'est une fonction cumulative : pour une valeur $x$ donnée, elle additionne les probabilités de tous les résultats inférieurs ou égaux à $x$.
La CDF a toujours une forme d'escalier pour les variables discrètes. Elle commence à 0 (très loin à gauche) et monte par "sauts" à chaque valeur possible de la variable, pour finalement atteindre 1 (très loin à droite). La hauteur de chaque saut correspond à la valeur de la PMF à ce point.
\end{intuitionbox}

Traçons cette fonction "en escalier" pour notre exemple du dé.

\begin{examplebox}
Reprenons le lancer d'un dé équilibré ($X$). Calculons quelques valeurs de la CDF, notée $F(x)$.

$F(0.5) = P(X \le 0.5) = 0$

$F(1) = P(X \le 1) = P(X=1) = 1/6$

$F(1.5) = P(X \le 1.5) = P(X=1) = 1/6$

$F(2) = P(X \le 2) = P(X=1) + P(X=2) = 2/6$

$F(5.9) = P(X \le 5.9) = P(X=1) + \dots + P(X=5) = 5/6$

$F(6) = P(X \le 6) = 1$

$F(100) = P(X \le 100) = 1$
\end{examplebox}

\subsection{Variable Aléatoire Indicatrice}

Enfin, nous introduisons un outil simple mais qui s'avérera extraordinairement puissant pour les preuves, notamment celles concernant l'espérance.

\begin{definitionbox}[Variable Aléatoire Indicatrice]
La variable aléatoire indicatrice d'un événement $A$ est la variable aléatoire qui vaut 1 si $A$ se produit et 0 sinon. Nous la noterons $I_A$. Notez que $I_A \sim \text{Bern}(p)$ avec $p=P(A)$.
\end{definitionbox}

C'est un simple interrupteur "on/off".

\begin{intuitionbox}
Une variable indicatrice est un interrupteur. Elle est sur "ON" (valeur 1) si un événement qui nous intéresse se produit, et sur "OFF" (valeur 0) sinon. C'est un outil extrêmement puissant car il transforme les questions sur les probabilités des événements en questions sur les espérances des variables aléatoires, ce qui simplifie souvent les calculs.
\end{intuitionbox}